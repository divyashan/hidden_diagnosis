{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# System imports\n",
    "import os\n",
    "import warnings\n",
    "import sys \n",
    "sys.path.append('../src/')\n",
    "\n",
    "# Custom imports\n",
    "from training import train\n",
    "from gpu_utils import restrict_GPU_pytorch\n",
    "\n",
    "# Configure settings\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "restrict_GPU_pytorch('0')\n",
    "np.random.seed(0)\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load extracted EDW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticoag_treatment = pd.read_csv('../extracted_EDW_data/trt_antic_ecglst.csv')\n",
    "anticoag_treatment['anticoag_treatment'] = ~anticoag_treatment['OrderDTS'].isna()\n",
    "anticoag_treatment = anticoag_treatment[['UniqueID', 'anticoag_treatment']]\n",
    "\n",
    "# Filter for earlest hospitalization that occurs after ecg_date_lst\n",
    "hospitalization = pd.read_csv('../extracted_EDW_data/hospital_vists.csv')\n",
    "hospitalization['HospitalAdmitDTS'] = pd.to_datetime(hospitalization['HospitalAdmitDTS'])\n",
    "hospitalization['ecg_date_lst'] = pd.to_datetime(hospitalization['ecg_date_lst'])\n",
    "hospitalization = hospitalization[hospitalization['HospitalAdmitDTS'] > hospitalization['ecg_date_lst']]\n",
    "hospitalization = hospitalization.sort_values('HospitalAdmitDTS')\n",
    "hospitalization = hospitalization.groupby('UniqueID').first().reset_index()\n",
    "\n",
    "specialist_visit = pd.read_csv('../extracted_EDW_data/specvis_lst.csv')\n",
    "specialist_visit = specialist_visit[['UniqueID', 'spec_vis']]\n",
    "\n",
    "insurance = pd.read_csv('../extracted_EDW_data/ins_ecglst.csv')\n",
    "insurance = insurance[['UniqueID', 'instype_final']]\n",
    "\n",
    "instype_map = {'Medicaid': 'Medicaid', 'Unknown/Missing': 'Unknown/Missing', 'Commercial': 'Commercial', 'Dual': 'Medicare', 'Medicare':'Medicare','Other': 'Other' }\n",
    "insurance['instype_final'] = insurance['instype_final'].map(instype_map)\n",
    "est_care = pd.read_csv('../extracted_EDW_data/estcare_ecglst.csv')\n",
    "est_care = est_care[['UniqueID', 'PCPvisits_bin', 'CARvisits_bin', 'OTHvisits_bin']]\n",
    "\n",
    "rate = pd.read_csv('../extracted_EDW_data/rate_ecglst.csv')\n",
    "rhythm = pd.read_csv('../extracted_EDW_data/rhythm_ecglst.csv')\n",
    "stroke = pd.read_csv('../extracted_EDW_data/stroke_ecglst.csv')\n",
    "stroke['ecg_date_lst'] = pd.to_datetime(stroke['ecg_date_lst'])\n",
    "stroke['DGNS_DT'] = pd.to_datetime(stroke['DGNS_DT'])\n",
    "\n",
    "stroke['stroke_within_year'] = (\n",
    "    (stroke['stroke'] == 1) & \n",
    "    ((stroke['DGNS_DT'] - stroke['ecg_date_lst']).dt.days <= 365)\n",
    ").astype(int)\n",
    "\n",
    "rate = rate[['UniqueID', 'trt_rate']]\n",
    "rhythm = rhythm[['UniqueID', 'trt_rhythm']]\n",
    "stroke = stroke[['UniqueID', 'stroke', 'stroke_within_year']]\n",
    "\n",
    "exists_in_new_system = pd.read_csv('../extracted_EDW_data/Missing Patients EDW RZ.csv')\n",
    "unique_ids_in_new_system = exists_in_new_system[exists_in_new_system['patid_found'] == 1]['UniqueID'].unique()\n",
    "\n",
    "\n",
    "rate['trt_rate'].mean(), rhythm['trt_rhythm'].mean(), stroke['stroke'].mean(), stroke['stroke_within_year'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load AF dataset and combine with EDW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import map_params_to_filename\n",
    "from ecg_datasets import ECGDemographicsDataset, ECGDataset\n",
    "from ecg_preprocessing_fs import create_name_dob_hash\n",
    "import pandas as pd \n",
    "\n",
    "outcome= 'afib'\n",
    "merge_with_EDW_vars = True\n",
    "preprocessing_params = {'max_pred_gap': 90, \n",
    "                        'selection_criteria': 'va', \n",
    "                        'include_single_ecgs': True, \n",
    "                        'mini': False}\n",
    "unique_id_col = 'UniqueID'\n",
    "\n",
    "\n",
    "ecg_data = pd.read_csv('./processed_data/processed_afib_' + map_params_to_filename(preprocessing_params) + '.csv')   \n",
    "ecg_data['PatientFirstName'].fillna('nan', inplace=True)\n",
    "ecg_data[unique_id_col] = create_name_dob_hash(ecg_data, 'PatientFirstName', 'PatientLastName', 'DateofBirth')\n",
    "\n",
    "print(\"# of (Patients, ECGs) before merging with map to UniqueID: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "\n",
    "preprocessing_params['one_ecg_per_patient'] = 'last' # can be 'false', 'last', 'first', 'last_white', 'last_two_ecgs'\n",
    "preprocessing_params['loss'] = 'CE' # can be CE or Focal\n",
    "preprocessing_params['mini'] = False # Refers to subsampling the train set.\n",
    "\n",
    "# Collated based on files that throw errors \n",
    "files_to_skip = ['/data/workspace/ekg_bwr_trunc_norm/003161595_08-05-2019_15-20-53_SCD10410491PA05082019152053.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/000122100_09-21-2019_01-02-14_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006363943_05-24-2018_18-00-22_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/010464725_05-24-2019_15-17-48_SCD12365371PA24052019151748.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/000951233_08-12-2019_15-28-53_SKJ14029684PA12082019152853.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/003145657_08-14-2019_11-01-13_SKJ13388441SA14082019110113.npy', \n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/035737121_08-13-2017_18-51-36_SKJ13408672SA13082017185136.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/003156598_11-22-2019_16-12-28_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-07-2019_08-28-52_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-11-2019_03-20-46_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-06-2019_23-35-11_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/001410670_04-19-2016_14-07-26_SCD06223397PA19042016140726.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002105344_08-10-2018_16-05-48_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002105344_08-10-2018_16-05-48_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002063599_01-26-2017_10-56-00_SCD07047035PA26012017105600.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/001410670_04-19-2016_14-07-26_SCD06223397PA19042016140726.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-06-2019_23-35-11_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-07-2019_01-21-07_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002120566_12-19-2018_08-39-41_SKJ14080390PA19122018083941.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002021971_05-14-2016_09-49-49_none.npy'\n",
    "                    ]\n",
    "\n",
    "ecg_data = ecg_data[~ecg_data['path_to_bwr_trunc_norm_data'].isin(files_to_skip)]\n",
    "ecg_data['ecg_date'] = pd.to_datetime(ecg_data[['year', 'month', 'day']])\n",
    "ecg_data['DateofBirth'] = pd.to_datetime(ecg_data['DateofBirth'])\n",
    "ecg_data['PatientAge'] = ecg_data['ecg_date'] - ecg_data['DateofBirth']\n",
    "ecg_data['PatientAge_years'] = ecg_data['PatientAge'].dt.days / 365.2425\n",
    "ecg_data['PatientAge_years_01'] = ecg_data['PatientAge_years'] / 100\n",
    "ecg_data.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'PatientID', 'year', 'month', 'day', 'muse_mrn'], inplace=True)\n",
    "\n",
    "\n",
    "print(\"# of (Patients, ECGs) before merging with map to UniqueID: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "muse_edw_map = pd.read_csv('../outputs/intermediate/muse_edw_map.csv', dtype='str')\n",
    "ecg_data = ecg_data[ecg_data['UniqueID'].isin(muse_edw_map['UniqueID'])]\n",
    "ecg_data = pd.merge(ecg_data, muse_edw_map[['UniqueID', 'PatientID']], on='UniqueID')\n",
    "print(\"# of (Patients, ECGs) after merging with map to UniqueID: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "\n",
    "\n",
    "# Demographics based on Brianna's pull\n",
    "demographics_file = pd.read_csv('../extracted_EDW_data/demographics_with_diagnosis_info.csv')\n",
    "demographics_file['earliest_diagnosis'] = pd.to_datetime(demographics_file['earliest_diagnosis'])\n",
    "\n",
    "test_set_size = .4\n",
    "random_state = 0\n",
    "preprocessing_params['test_set_size'] = test_set_size\n",
    "# preprocessing_params['random_state'] = random_state\n",
    "\n",
    "pids = sorted(list(set(ecg_data[unique_id_col])))\n",
    "train_ids, test_ids = train_test_split(pids, test_size=preprocessing_params['test_set_size'], random_state=random_state)\n",
    "val_ids, test_ids = train_test_split(test_ids, test_size=.5, random_state=random_state)\n",
    "\n",
    "if merge_with_EDW_vars:\n",
    "    # Merge with demographics\n",
    "    ecg_data = ecg_data[ecg_data['UniqueID'].isin(demographics_file['UniqueID'])]\n",
    "    print(\"# of (Patients, ECGs) after demographics merge: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "    ecg_data = pd.merge(ecg_data, demographics_file, on='UniqueID')\n",
    "\n",
    "\n",
    "    print(\"# of Patients in sample matched to some diagnosis: \", ecg_data['diagnosis_in_charts'].sum())\n",
    "    # Filter for rows where diagnosis occurs AFTER ECG or there is no diagnosis in the charts\n",
    "    ecg_data = ecg_data[(~ecg_data['diagnosis_in_charts']) | (0 < (ecg_data['earliest_diagnosis']  - ecg_data['ecg_date']).dt.days)]\n",
    "    print(\"# of (Patients, ECGs) after filtering out established AFib diagnoses: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "    ecg_data['time_to_diagnosis'] = (ecg_data['earliest_diagnosis'] - ecg_data['ecg_date']).dt.days\n",
    "\n",
    "    # Add binary indicators for demographics\n",
    "    for race_val in ['White', 'Hispanic or Latino', 'Black or African American', 'Asian', 'Other',\n",
    "                     'Declined or Unavailable', 'Native American or Pacific Islander']:\n",
    "        ecg_data['binary_' + race_val] = ecg_data['PatientRaceFinal'] == race_val\n",
    "    ecg_data['binary_Male'] = ecg_data['SexDSC'] == 'Male'\n",
    "    ecg_data.drop(columns=['binary_Race_CAUCASIAN', 'binary_Race_HISPANIC', 'binary_Race_BLACK', 'binary_Race_HISPANIC'], inplace=True)\n",
    "\n",
    "    # Add indicators for downstream outcomes\n",
    "    ecg_data = pd.merge(ecg_data, anticoag_treatment, on='UniqueID') # 13% of patients go on to have anticoag treatment \n",
    "    ecg_data['mortality'] = ~ecg_data['DeathDTS'].isna() # 12% of patients die\n",
    "    ecg_data['hospitalization'] = ecg_data['UniqueID'].isin(hospitalization['UniqueID']) \n",
    "    ecg_data = pd.merge(ecg_data, specialist_visit, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, rate, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, rhythm, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, stroke, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, est_care, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, insurance, on='UniqueID')\n",
    "print(\"\\n# of Patients in  sample: \", ecg_data['UniqueID'].nunique())\n",
    "print(\"# of ECGs in sample: \", len(ecg_data))\n",
    "\n",
    "ecg_data['DeathDTS'] = pd.to_datetime(ecg_data['DeathDTS']) \n",
    "ecg_data['date'] = pd.to_datetime(ecg_data['date']) \n",
    "ecg_data['mortality_within_one_year'] =  (ecg_data['DeathDTS'] - ecg_data['date']).dt.days < 365\n",
    "\n",
    "# Remove UniqueIDs who are associated with; both positive & negative class;\n",
    "# it's because sometime a patient's name includes their middle name,\n",
    "# which can result in two UniqueIDs for the same patient.\n",
    "uniqueids_positive_and_negative = ecg_data.groupby('UniqueID')['label'].nunique()\n",
    "repeated_unique_ids_across_class = uniqueids_positive_and_negative[uniqueids_positive_and_negative > 1].index.values\n",
    "ecg_data = ecg_data[~ecg_data['UniqueID'].isin(repeated_unique_ids_across_class)]\n",
    "\n",
    "if preprocessing_params['one_ecg_per_patient'] != 'false':\n",
    "    # Sort by ECG date (ascending=False)\n",
    "    # Select one per UniqueID\n",
    "    ecg_data = ecg_data.sort_values('ecg_date')\n",
    "    if preprocessing_params['one_ecg_per_patient'].startswith('first'):\n",
    "        ecg_data = ecg_data.groupby('UniqueID').first().reset_index()\n",
    "    else:\n",
    "        ecg_data = ecg_data.groupby('UniqueID').last().reset_index()\n",
    "print(ecg_data['label'].mean(), preprocessing_params)\n",
    "print(\"\\n# of Patients in  sample: \", ecg_data['UniqueID'].nunique())\n",
    "print(\"# of ECGs in sample: \", len(ecg_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sensitivity analysis, is not used in main analysis.\n",
    "if preprocessing_params['one_ecg_per_patient'] == 'last_two_ecgs':\n",
    "    negatives = ecg_data[ecg_data['label'] == 0]\n",
    "    muse_cache_df = pd.read_csv('../outputs_intermediate/muse_cache_files/patient_mrn_to_file.csv', dtype='str')\n",
    "\n",
    "    all_negatives_ecgs = muse_cache_df[muse_cache_df['UniqueID'].isin(negatives['UniqueID'])]\n",
    "    all_negatives_ecgs['date'] = pd.to_datetime(all_negatives_ecgs[['year', 'month', 'day']])\n",
    "    all_negatives_ecgs = all_negatives_ecgs.sort_values(by = 'date')\n",
    "    all_negatives_ecgs = all_negatives_ecgs.groupby('UniqueID').head(2).reset_index()\n",
    "\n",
    "    # Filter out  patients with only 1 ECG\n",
    "    unique_id_to_counts = all_negatives_ecgs['UniqueID'].value_counts() \n",
    "    unique_id_to_counts =  unique_id_to_counts[unique_id_to_counts > 1]\n",
    "    print(\"# of Unique IDs in negatives: \", all_negatives_ecgs['UniqueID'].nunique())\n",
    "    all_negatives_ecgs = all_negatives_ecgs[all_negatives_ecgs['UniqueID'].isin(unique_id_to_counts.index)]\n",
    "    print(\"# of Unique IDs in negatives after filtering out patients with 1 ECG: \",all_negatives_ecgs['UniqueID'].nunique())\n",
    "\n",
    "    # Filter out patients where second ECG happens more than 90 days after first\n",
    "    all_negatives_ecgs = all_negatives_ecgs.sort_values(by=['UniqueID', 'date'])\n",
    "\n",
    "    # Compute the difference in days between the two rows for each UniqueID\n",
    "    date_diff = all_negatives_ecgs.groupby('UniqueID')['date'].diff().abs()\n",
    "\n",
    "    # Keep only UniqueIDs where the date difference is â‰¤ 90 days\n",
    "    valid_ids = date_diff.groupby(all_negatives_ecgs['UniqueID']).max().between(pd.Timedelta(days=1), pd.Timedelta(days=89))\n",
    "\n",
    "    # Filter the dataframe to retain only valid UniqueIDs\n",
    "    all_negatives_ecgs = all_negatives_ecgs[all_negatives_ecgs['UniqueID'].isin(valid_ids[valid_ids].index)]\n",
    "    print(\"# of Unique IDs in negatives after filtering for patients with 2 ECGs within 90 days \",all_negatives_ecgs['UniqueID'].nunique())\n",
    "\n",
    "    all_negatives_ecgs = all_negatives_ecgs.sort_values(by='date')\n",
    "    negative_pids_with_two_ecgs =  all_negatives_ecgs['UniqueID'].values\n",
    "    positive_pids = ecg_data[ecg_data['label'] == 1]['UniqueID'].values\n",
    "    pids_to_keep_under_two_ecg_constraint = np.concatenate([negative_pids_with_two_ecgs, positive_pids])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split dataset into train, calibration, and study sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "N_WORKERS = 6\n",
    "NUM_EPOCHS = 10\n",
    "LR = 0.0001\n",
    "\n",
    "unique_id_col = 'UniqueID'\n",
    "additional_feat_names = ['binary_Black or African American', 'binary_Hispanic or Latino',\n",
    "                         'binary_Declined or Unavailable','binary_Asian',  'binary_Other', \n",
    "                         'binary_Native American or Pacific Islander',\n",
    "                         'binary_Male', \n",
    "                         'PatientAge_years_01']\n",
    "\n",
    "split_dfs = []\n",
    "split_paths = []\n",
    "split_y = []\n",
    "additional_feats = []\n",
    "for i, pid_set in enumerate([train_ids, val_ids, test_ids]):\n",
    "    split_df = ecg_data[ecg_data[unique_id_col].isin(pid_set)]\n",
    "    split_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Relevant to sensitivity analyses; not used in main analysis.\n",
    "    if i in [0,1] and preprocessing_params['one_ecg_per_patient'] == 'last_white':\n",
    "        split_df = split_df[split_df['PatientRaceFinal'] == 'White']\n",
    "    \n",
    "    # Relevant to sensitivity analyses; not used in main analysis.\n",
    "    if i in [0,1] and preprocessing_params['one_ecg_per_patient'] == 'last_two_ecgs':\n",
    "        split_df = split_df[split_df['UniqueID'].isin(pids_to_keep_under_two_ecg_constraint)]\n",
    "\n",
    "    if i > 0:\n",
    "        # Ensure we evaluate each patient with only one ECG\n",
    "        split_df = split_df.sample(frac=1, random_state=0)\n",
    "        split_df = split_df.groupby(unique_id_col).first().reset_index() # Replace with a random ECG\n",
    "        print(split_df[unique_id_col].nunique())\n",
    "        split_df = split_df[split_df[unique_id_col].isin(unique_ids_in_new_system)]\n",
    "        print(split_df[unique_id_col].nunique())\n",
    "\n",
    "    split_paths.append(list(split_df['path_to_bwr_trunc_norm_data']))\n",
    "    split_y.append(np.array(list(split_df['label'])))\n",
    "    additional_feat_values = split_df[additional_feat_names].fillna(0).values\n",
    "    split_dfs.append(split_df)\n",
    "    additional_feats.append(additional_feat_values.astype(int))\n",
    "\n",
    "train_ecg_paths, val_ecg_paths, test_ecg_paths = split_paths\n",
    "train_additional_feats, val_additional_feats, test_additional_feats = additional_feats\n",
    "train_y, val_y, test_y = split_y\n",
    "\n",
    "if len(additional_feat_names) == 0:\n",
    "    train_dataset = ECGDataset(train_ecg_paths, train_y)\n",
    "    val_dataset = ECGDataset(val_ecg_paths, val_y)\n",
    "    test_dataset = ECGDataset(test_ecg_paths, test_y)\n",
    "else:\n",
    "    train_dataset = ECGDemographicsDataset(train_ecg_paths,  train_additional_feats, train_y)\n",
    "    val_dataset = ECGDemographicsDataset(val_ecg_paths, val_additional_feats, val_y)\n",
    "    test_dataset = ECGDemographicsDataset(test_ecg_paths, test_additional_feats, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=False, shuffle=True, num_workers=N_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, pin_memory=False, shuffle=False, num_workers=N_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=False, shuffle=False, num_workers=N_WORKERS)\n",
    "print(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset))\n",
    "\n",
    "expt_config={'arch': 'Net1D','additional_features': False}\n",
    "if len(additional_feat_names) > 0:\n",
    "    expt_config['additional_features'] = True\n",
    "\n",
    "print(\"AF rates across splits: \", train_y.mean(), val_y.mean(), test_y.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_model\n",
    "from torch import optim, nn\n",
    "from kornia.losses import BinaryFocalLossWithLogits\n",
    "\n",
    "model = get_model(additional_inputs=0)\n",
    "if expt_config['additional_features']:\n",
    "    model = get_model( additional_inputs=len(additional_feat_names)) # 2.8M params\n",
    "\n",
    "# Obtain from: https://github.com/ecg-net/PreOpNet.\n",
    "checkpoint = torch.load('../PreOpNet/PreOpNet MACE.pt')\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "model.cuda()\n",
    "\n",
    "# Option to use a focal loss instead of cross-entropy. We found it did not \n",
    "# improve calibration of the network trained on the ECG + demographics. \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "if preprocessing_params['loss'] == 'Focal':\n",
    "    kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n",
    "    criterion =  BinaryFocalLossWithLogits(**kwargs)\n",
    "\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=LR) # Same as VA paper\n",
    "\n",
    "save_file_name = '../outputs/saved_models/model_tmp_' + outcome \n",
    "if expt_config['additional_features']:\n",
    "    save_file_name = './outputs/saved_models/model_tmp_' + outcome + '_with_other_features'\n",
    "\n",
    "preprocessing_keys = sorted(preprocessing_params.keys())\n",
    "for key in preprocessing_keys:\n",
    "    save_file_name = save_file_name +'_' + key + '=' + str(preprocessing_params[key])\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(expt_config)\n",
    "print(save_file_name)\n",
    "\n",
    "model, history = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer_ft,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    save_file_name,\n",
    "    max_epochs_stop=10,\n",
    "    n_epochs=NUM_EPOCHS,\n",
    "    print_every=1,\n",
    "    augmentations_on=True,\n",
    "    expt_config=expt_config)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_model\n",
    "import statsmodels.api as sm\n",
    "from utils import calibration_plot, diagnosis_curve_plot\n",
    "\n",
    "if expt_config['additional_features']:\n",
    "    model = get_model(additional_inputs=len(additional_feat_names)) # 2.8M params\n",
    "    save_file_name = './outputs/saved_models/model_tmp_' + outcome + '_with_other_features'\n",
    "else:\n",
    "    model = get_model()\n",
    "    save_file_name = './outputs/saved_models/model_tmp_' + outcome\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_keys = sorted(preprocessing_params.keys())\n",
    "for key in preprocessing_keys:\n",
    "    save_file_name = save_file_name +'_' + key + '=' + str(preprocessing_params[key])\n",
    "\n",
    "\n",
    "print(save_file_name)\n",
    "checkpoint = torch.load(save_file_name)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Obtain model predictions on calibration and study sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda:{}\".format(0))\n",
    "    all_pred_prob = []\n",
    "    all_pred = []\n",
    "    all_labels = []\n",
    "    for data, target in tqdm(dataloader):\n",
    "        if len(data) == 2:\n",
    "            data = (data[0].to(device=device, dtype=torch.float), data[1].to(device=device, dtype=torch.float))\n",
    "            cleaned_data = (torch.nan_to_num(data[0], nan=0), torch.nan_to_num(data[1], nan=0))\n",
    "        else:\n",
    "            data = data.to(device=device, dtype=torch.float)\n",
    "            cleaned_data = torch.nan_to_num(data, nan=0) \n",
    "\n",
    "        cleaned_targets = torch.nan_to_num(target, nan=0)\n",
    "        output = model(cleaned_data)\n",
    "        pred_prob = torch.sigmoid(output)\n",
    "        pred = torch.round(torch.sigmoid(output))\n",
    "        all_pred_prob.append(pred_prob.detach())\n",
    "        all_pred.append(pred.detach())\n",
    "        all_labels.append(cleaned_targets.detach())\n",
    "\n",
    "    # Calculate AUC\n",
    "    all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "    all_pred = torch.cat(all_pred).cpu().numpy()\n",
    "    all_pred_prob = torch.cat(all_pred_prob).cpu().numpy()\n",
    "    acc = np.mean(all_labels == all_pred)\n",
    "    \n",
    "    sensitivity = np.mean(all_pred[all_labels == 1])\n",
    "    specificity = 1 - np.mean(all_pred[all_labels == 0])\n",
    "    auc = roc_auc_score(all_labels, all_pred_prob)\n",
    "    auprc = average_precision_score(all_labels, all_pred_prob)\n",
    "    return acc, auc, auprc, sensitivity, specificity, all_labels, all_pred_prob, all_pred\n",
    "\n",
    "acc, auc, auprc, _, _, val_labels, val_pred_probs, val_preds = eval(model, val_loader)\n",
    "print('Accuracy: ', acc, '\\tAUPRC: ', auprc, '\\tAUC: ', auc)\n",
    "acc, auc, auprc, sensitivity, specificity, test_labels, test_pred_probs, test_preds = eval(model, test_loader)\n",
    "acc, auc, auprc, sensitivity, specificity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calibrate model using the calibration sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "age_thresh = 17\n",
    "val_split_df = split_dfs[1]\n",
    "test_split_df = split_dfs[2]\n",
    "train_split_df = split_dfs[0]\n",
    "\n",
    "val_split_df['risk_score'] = val_pred_probs\n",
    "test_split_df['risk_score'] = test_pred_probs\n",
    "\n",
    "# For sensitivity analysis; not used in main analysis.\n",
    "if preprocessing_params['one_ecg_per_patient'] != 'last_two_ecgs':\n",
    "    combined_df = pd.concat([val_split_df, test_split_df], ignore_index=True)\n",
    "    shuffled_df = shuffle(combined_df, random_state=42)\n",
    "\n",
    "    # Split back into validation and test sets\n",
    "    split_ratio = 0.625\n",
    "    split_point = int(len(shuffled_df) * split_ratio)\n",
    "\n",
    "    # Create new validation and test splits\n",
    "    val_split_df = shuffled_df.iloc[:split_point].reset_index(drop=True)\n",
    "    test_split_df = shuffled_df.iloc[split_point:].reset_index(drop=True)\n",
    "\n",
    "if preprocessing_params['one_ecg_per_patient'] == 'last_white':\n",
    "    val_split_df = val_split_df[val_split_df['PatientRaceFinal'] == 'White']\n",
    "\n",
    "\n",
    "val_split_df['instype_final'] = val_split_df['instype_final'].map(instype_map)\n",
    "val_split_df = val_split_df[val_split_df['PatientAge_years'] > age_thresh]\n",
    "val_split_df = val_split_df[val_split_df['instype_final'].isin(['Medicare', 'Medicaid', 'Commercial'])]\n",
    "val_split_df = val_split_df[val_split_df['PatientRaceFinal'].isin(['White', 'Black or African American', 'Hispanic or Latino', 'Asian', 'Other'])]\n",
    "val_split_df = val_split_df[val_split_df['PrimLangDSC'].isin(['English', 'Spanish', 'Other'])]\n",
    "\n",
    "\n",
    "test_split_df = test_split_df[test_split_df['PatientAge_years'] > age_thresh]\n",
    "test_split_df = test_split_df[test_split_df['instype_final'].isin(['Medicare', 'Medicaid', 'Commercial'])]\n",
    "test_split_df = test_split_df[test_split_df['PatientRaceFinal'].isin(['White', 'Black or African American', 'Hispanic or Latino', 'Asian', 'Other'])]\n",
    "test_split_df = test_split_df[test_split_df['PrimLangDSC'].isin(['English', 'Spanish', 'Other'])]\n",
    "test_split_df['instype_final'] = test_split_df['instype_final'].map(instype_map)\n",
    "\n",
    "def add_necessary_binary_features(df):\n",
    "    df['binary_Female'] = df['SexDSC'] == 'Female'\n",
    "    df['binary_Non-English'] = df['PrimLangDSC'] != 'English'\n",
    "    df['binary_Medicare'] = df['instype_final'] == 'Medicare'\n",
    "    df['binary_Medicaid'] = df['instype_final'] == 'Medicaid'\n",
    "    df['binary_NoPCPEncounter'] = df['PCPvisits_bin'] == 0\n",
    "    df['binary_NoCAREncounter'] = df['CARvisits_bin'] == 0\n",
    "    df['binary_NoOTHEncounter'] = df['OTHvisits_bin'] == 0\n",
    "    return  df\n",
    "    \n",
    "val_split_df = add_necessary_binary_features(val_split_df)\n",
    "test_split_df = add_necessary_binary_features(test_split_df)\n",
    "train_split_df = add_necessary_binary_features(train_split_df)\n",
    "\n",
    "test_split_df['PrimLangDSC'] = test_split_df['PrimLangDSC'].map(lambda x: x if x == \"English\" else \"Non-English\")\n",
    "\n",
    "\n",
    "lr = LogisticRegression(penalty=None)\n",
    "lpm = LinearRegression()\n",
    "feat_names = [\n",
    "            'binary_Black or African American', 'binary_Hispanic or Latino', 'binary_Other', 'binary_Asian', \n",
    "            'binary_Medicaid', 'binary_Medicare', 'binary_Non-English',\n",
    "            'binary_NoPCPEncounter', 'binary_NoCAREncounter', 'binary_NoOTHEncounter',\n",
    "            'binary_Female', 'PatientAge_years_01', 'risk_score' ]\n",
    "\n",
    "val_X_Y = val_split_df[feat_names + ['label']]\n",
    "val_X = val_X_Y[feat_names].values.astype(float)\n",
    "val_y = val_X_Y['label'].values.astype(float)\n",
    "\n",
    "test_X_Y = test_split_df[feat_names + ['label']]\n",
    "test_X = test_X_Y[feat_names].values.astype(float)\n",
    "test_y = test_X_Y['label'].values.astype(float)\n",
    "\n",
    "val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "test_X_Y = test_split_df[feat_names + ['label']].dropna()\n",
    "test_X = test_X_Y[feat_names].values.astype(float)\n",
    "test_y = test_X_Y['label'].values.astype(float)\n",
    "\n",
    "test_X = scaler.transform(test_X)\n",
    "\n",
    "lr.fit(val_X, val_y)\n",
    "test_lr_pred_probs = lr.predict_proba(test_X)[:,1]\n",
    "val_split_df['lr_adjusted_risk_score'] = lr.predict_proba(val_X)[:,1]\n",
    "test_split_df['lr_adjusted_risk_score'] = test_lr_pred_probs\n",
    "\n",
    "lpm.fit(val_X, val_y)\n",
    "val_split_df['lpm_adjusted_risk_score'] = np.clip(lpm.predict(val_X), 0, 1)\n",
    "test_split_df['lpm_adjusted_risk_score'] = np.clip(lpm.predict(test_X), 0, 1)\n",
    "\n",
    "\n",
    "print(roc_auc_score(val_split_df['label'], val_split_df['lr_adjusted_risk_score']), roc_auc_score(test_split_df['label'], test_split_df['lr_adjusted_risk_score']))\n",
    "print(roc_auc_score(val_split_df['label'], val_split_df['lpm_adjusted_risk_score']),roc_auc_score(test_split_df['label'], test_split_df['lpm_adjusted_risk_score']))\n",
    "\n",
    "# Verify that the risk score is calibrated with respect to the label using an LPM fit to the study sample.\n",
    "demographic_variables =  [\n",
    "                        'binary_Black or African American', 'binary_Hispanic or Latino',\n",
    "                         'binary_Asian',  'binary_Other', \n",
    "                         'binary_Medicare', 'binary_Medicaid', \n",
    "                         'binary_Non-English',\n",
    "                         'binary_Female', \n",
    "                         'PatientAge_years_01',\n",
    "                        'binary_NoPCPEncounter', 'binary_NoCAREncounter', 'binary_NoOTHEncounter'\n",
    "\n",
    "                         ]\n",
    "\n",
    "outcome = 'label'\n",
    "for risk_score_col_name in ['lpm_adjusted_risk_score']:\n",
    "    # Print out the statistics\n",
    "    df = test_split_df.copy()\n",
    "    df = df[demographic_variables + [risk_score_col_name, outcome]].dropna()\n",
    "\n",
    "    X = df.drop(outcome, axis=1)  # Independent variables\n",
    "    X = X[[risk_score_col_name] + demographic_variables ].astype(float)\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    y = df[outcome].astype(int)  # Dependent variable\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "\n",
    "    results = model.fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Measure performance of risk score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 AUC plots (Figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roc_utils as ru\n",
    "from utils import colors, prettify_group_name\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# To perform a ROC analysis using bootstrapping\n",
    "n_samples = 10\n",
    "\n",
    "axis_params = [('PatientRaceFinal', ('White', 'Black or African American', 'Hispanic or Latino', 'Asian')),\n",
    "               ('instype_final', ('Commercial', 'Medicare', 'Medicaid')),\n",
    "                ('PrimLangDSC', ('English', 'Non-English')),]\n",
    "\n",
    "n_cols = 3 \n",
    "n_rows = 2\n",
    "risk_score_col_name = 'lpm_adjusted_risk_score'\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize=(15, 8), dpi=300)  # Adjust the figsize as needed\n",
    "\n",
    "# Define a gridspec layout for consistent subplot widths\n",
    "gs = gridspec.GridSpec(2, 3, height_ratios=[2, 1], hspace=0.2)\n",
    "\n",
    "# Top row of plots\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "# Bottom row of plots\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "axs = [[ax1, ax2, ax3], [ax4, ax5, ax6]]\n",
    "for i, (col_name, feature_names) in enumerate(axis_params):\n",
    "    ax = axs[0][i]\n",
    "    plt.sca(ax)\n",
    "\n",
    "    for feature_name in feature_names:\n",
    "\n",
    "        plot_df = test_split_df[test_split_df[col_name] == feature_name]\n",
    "        x = plot_df[risk_score_col_name]\n",
    "        y = plot_df['label']\n",
    "        roc = ru.compute_roc(X=x, y=y, pos_label=1)\n",
    "\n",
    "        ru.plot_roc(roc,color=colors[feature_name], label=feature_name)\n",
    "\n",
    "    \n",
    "    plt.title(prettify_group_name[col_name])\n",
    "    if i!=0:\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "### Plot event rate in different groups\n",
    "axis_params = [('PatientRaceFinal', ('White', 'Black or African American', 'Hispanic or Latino', 'Asian')),\n",
    "        ('instype_final', ('Commercial', 'Medicare', 'Medicaid')),\n",
    "        ('PrimLangDSC', ('English', 'Non-English')),\n",
    "        ]\n",
    "\n",
    "for k, (group_name, group_vals) in enumerate(axis_params):\n",
    "    ax = axs[1][k]\n",
    "    plt.sca(ax)\n",
    "\n",
    "    plot_estimates = test_split_df.groupby(group_name)[[risk_score_col_name, 'label']].mean().reset_index()\n",
    "    plot_estimates = plot_estimates[plot_estimates[group_name].isin(group_vals)]\n",
    "    plot_estimates = pd.melt(plot_estimates, id_vars=[group_name], value_vars=[risk_score_col_name, 'label'], var_name='Type', value_name='Estimate')\n",
    "    plot_estimates['Type'] = plot_estimates['Type'].map({risk_score_col_name: 'Predicted', 'label': 'Empirical'})\n",
    "    if group_name == 'PatientRaceFinal':\n",
    "        plot_estimates['PatientRaceFinal'] = pd.Categorical(plot_estimates[group_name], categories=['White', 'Black or African American', 'Hispanic or Latino', 'Other', 'Asian'], ordered=True)\n",
    "        plot_estimates.sort_values('PatientRaceFinal', inplace=True)\n",
    "    elif group_name == 'instype_final':\n",
    "        plot_estimates['instype_final'] = pd.Categorical(plot_estimates[group_name], categories=['Commercial', 'Medicare', 'Medicaid'], ordered=True)\n",
    "        plot_estimates.sort_values('instype_final', inplace=True)\n",
    "\n",
    "    plot_estimates['Estimate'] = plot_estimates['Estimate']*100\n",
    "\n",
    "    for i, type in enumerate(['Predicted', 'Empirical']):\n",
    "    \n",
    "        if type == 'Empirical':\n",
    "            colname = 'label'\n",
    "            color = 'white'\n",
    "            edgecolor = 'black'\n",
    "        else:\n",
    "            colname = risk_score_col_name\n",
    "            color = 'black'\n",
    "            edgecolor = 'black'\n",
    "        values = plot_estimates[plot_estimates['Type'] == type]['Estimate'].values\n",
    "        group_vals = plot_estimates[plot_estimates['Type'] == type][group_name].values\n",
    "        yticks = list(range(len(values)))\n",
    "\n",
    "        for j, (value, ytick, group_val) in enumerate(zip(values, yticks, group_vals)):\n",
    "            group_color = colors[group_val]\n",
    "            type_name = type\n",
    "            if j != 0:\n",
    "                type_name = \"\"\n",
    "            if type == 'Empirical':\n",
    "                ax.scatter(value, ytick, s=100, facecolor=color,  edgecolor=group_color, label=type_name)\n",
    "            else:\n",
    "                ax.scatter(value, ytick, s=100, facecolor=group_color,  edgecolor=group_color, label=type_name, alpha=0.7)\n",
    "\n",
    "    ax.set_yticklabels([])\n",
    "    # ax.set_title(prettify_group_name[group_name])\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('AF ECG Prevalence (%)')\n",
    "\n",
    "    ax.set_ylim(np.min(yticks)-.5, np.max(yticks)+.5)\n",
    "    ax.set_xlim(0, 4.5)\n",
    "    ax.legend(loc='lower right', title='Event Rate')\n",
    "    if k != 2:\n",
    "        ax.legend().remove()\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlim(left=0)\n",
    "    # ax.set_xlim(right=0.05)\n",
    "    ax.grid(color='lightgray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "n_bins = 5\n",
    "test_split_df['quintile'] = pd.qcut(test_split_df[risk_score_col_name], q=n_bins, labels=False, duplicates='drop')\n",
    "\n",
    "# Get top and bottom quintiles\n",
    "top_quintile = test_split_df[test_split_df['quintile'] == test_split_df['quintile'].max()]\n",
    "bottom_quintile = test_split_df[test_split_df['quintile'] == test_split_df['quintile'].min()]\n",
    "\n",
    "# Compute rates\n",
    "top_rate = top_quintile['label'].mean()\n",
    "bottom_rate = bottom_quintile['label'].mean()\n",
    "\n",
    "# Compute ratio\n",
    "ratio = top_rate / bottom_rate if bottom_rate > 0 else float('inf')\n",
    "\n",
    "print(f\"Rate Ratio bt highest and lowest bins, ({n_bins} Bins): {ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Calibration plots (Figure S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import colors\n",
    "bins = np.linspace(0, 0.03, num=10)  # 0 to 0.5 with 10 equal-width bins\n",
    "\n",
    "# Create a placeholder for plotting data\n",
    "\n",
    "# Group by race\n",
    "group_name = 'PatientRaceFinal'\n",
    "group_vals = ['White', 'Black or African American', 'Hispanic or Latino', 'Asian', 'Other']\n",
    "\n",
    "# group_name = 'instype_final'\n",
    "# group_vals = ['Medicare', 'Medicaid', 'Commercial']\n",
    "\n",
    "# group_name = 'PrimLangDSC'\n",
    "# group_vals = ['English', 'Spanish', 'Other']\n",
    "\n",
    "axis_params = [('PatientRaceFinal', ('White', 'Black or African American', 'Hispanic or Latino', 'Asian')),\n",
    "               ('instype_final', ('Commercial', 'Medicare', 'Medicaid')),\n",
    "                ('PrimLangDSC', ('English', 'Non-English')),]\n",
    "\n",
    "n_cols = 3 \n",
    "n_rows = 1\n",
    "fig, axs = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(10, 4))\n",
    "\n",
    "risk_score_col_name = 'lpm_adjusted_risk_score'\n",
    "for i, (group_name, group_vals) in enumerate(axis_params):\n",
    "\n",
    "    plt.sca(axs[i])\n",
    "    plot_data = []\n",
    "\n",
    "    for group_val in group_vals:\n",
    "        group_df = test_split_df[test_split_df[group_name] == group_val]\n",
    "        # Bin the data\n",
    "        group_df['bin'] = pd.cut(group_df[risk_score_col_name], bins, include_lowest=True)\n",
    "        grouped = group_df.groupby('bin')\n",
    "        \n",
    "        # Calculate x and y values for the plot\n",
    "        x_values = grouped[risk_score_col_name].mean()\n",
    "        # y_values = grouped['label'].mean() - grouped[risk_score_col_name].mean()\n",
    "        y_values = grouped['label'].mean()\n",
    "        sizes = np.sqrt(grouped.size())  # Number of points in each bin\n",
    "        valid_bins = sizes[sizes > 20].index\n",
    "        x_values = x_values[valid_bins]\n",
    "        y_values = y_values[valid_bins]\n",
    "        sizes = sizes[valid_bins]\n",
    "        \n",
    "        # Add to plot data\n",
    "        plot_data.append((x_values, y_values, sizes, group_val))\n",
    "\n",
    "\n",
    "    for x, y, sizes, group_val in plot_data:\n",
    "        plt.scatter(\n",
    "            x, \n",
    "            y, \n",
    "            s=sizes,  # Marker size proportional to the group size\n",
    "            label=group_val, \n",
    "            color=colors.get(group_val, 'gray'), \n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "        # Add plot labels and legend\n",
    "    # plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    lower = -.002\n",
    "    upper = 0.03\n",
    "    plt.plot([lower, upper], [lower, upper], linestyle='--', linewidth=1, color='gray', alpha=0.7, zorder=-100)\n",
    "    plt.xlabel('Mean Predicted Risk Score')\n",
    "    plt.ylabel('Prediction Error\\n(Empirical Rate - Predicted Rate)')\n",
    "    if i > 0:\n",
    "        plt.ylabel('')\n",
    "    prettify_group_name = {'PatientRaceFinal': 'Race', 'instype_final': 'Insurance', 'PrimLangDSC': 'Primary Language'}\n",
    "    plt.title(\"Calibration by\\n{}\".format(prettify_group_name[group_name]))\n",
    "    plt.legend( loc='lower right', fontsize=8)\n",
    "    # plt.ylim(-.10, .10)\n",
    "    plt.xlim(lower, upper)\n",
    "    plt.ylim(lower, upper)\n",
    "    plt.grid(color='lightgray', linestyle='--')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Calibration statistical tests (Section 4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaleva import CalibrationEvaluator\n",
    "\n",
    "for i, (col_name, feature_names) in enumerate(axis_params):\n",
    "\n",
    "    for feature_name in feature_names:\n",
    "        print()\n",
    "        plot_df = test_split_df[test_split_df[col_name] == feature_name]\n",
    "        x = plot_df['lpm_adjusted_risk_score']\n",
    "        y = plot_df['label']\n",
    "\n",
    "        ce = CalibrationEvaluator(y, x, outsample=True, n_groups='auto')\n",
    "        print(prettify_group_name[col_name], ':',  feature_name, '\\n', ce.z_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Run main analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Main analysis set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dem_feat_names, ecg_feat_names\n",
    "from utils import plot_CIs_covariates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "table_name_target_col_map = {'diag': 'Diagnosis',\n",
    "                             'diag_1yr': 'Diagnosis, 1yr',\n",
    "                            'ecg': 'AF ECG',\n",
    "                            'rvr': 'HR > 160',\n",
    "                            'stroke': 'Stroke', \n",
    "                            'stroke_within_year': 'Stroke'}\n",
    "\n",
    "preprocessing_params['rar_covariates'] = 'demographics_alone' # 'demographics_alone, demographics_joint\n",
    "preprocessing_params['rar_model'] = 'OLS' # OLS or Logit\n",
    "preprocessing_params['risk_score_col_name'] = 'lpm_adjusted_risk_score'\n",
    "\n",
    "def convert_results_to_CI_df(results):\n",
    "    coefficients = results.params\n",
    "    errors = results.bse\n",
    "\n",
    "    # Create the confidence intervals\n",
    "    lower_bound = coefficients - 1.96 * errors\n",
    "    upper_bound = coefficients + 1.96 * errors\n",
    "\n",
    "    # Sort variables based on coefficients for cleaner visualization (optional)\n",
    "    variables = coefficients.index\n",
    "    sorted_idx = np.argsort(coefficients)\n",
    "    variables = variables[sorted_idx]\n",
    "    coefficients = coefficients[sorted_idx]\n",
    "    lower_bound = lower_bound[sorted_idx]\n",
    "    upper_bound = upper_bound[sorted_idx]\n",
    "\n",
    "    coefficients = pd.DataFrame(coefficients).rename(columns={0: 'Estimate'})\n",
    "    lower_bound = pd.DataFrame(lower_bound).rename(columns={0: 'Lower bound'})\n",
    "    upper_bound = pd.DataFrame(upper_bound).rename(columns={0: 'Upper bound'})\n",
    "    CI_df = pd.concat([coefficients, lower_bound, upper_bound], axis=1)\n",
    "    return CI_df\n",
    "\n",
    "## Add in HR > 160\n",
    "max_hr_window = 365\n",
    "## NOTE: Make sure to run Compute HR > 160 cell at the bottom of the notebook otherwise this will fail.\n",
    "unique_id_to_max_hr = pd.read_csv('unique_id_to_max_hr_' + str(max_hr_window)+ '_days')\n",
    "test_split_df['max_hr_within_' + str(max_hr_window) + '_days'] = test_split_df['UniqueID'].map(dict(zip(unique_id_to_max_hr['UniqueID'], unique_id_to_max_hr['AtrialRate'])))\n",
    "test_split_df['max_hr_within_365_days'].fillna(0, inplace=True)\n",
    "test_split_df['diagnosis_within_3yr'] = (test_split_df['diagnosis_in_charts'] & (test_split_df['time_to_diagnosis'] <= 1095)).astype(int)\n",
    "test_split_df['diagnosis_within_2yr'] = (test_split_df['diagnosis_in_charts'] & (test_split_df['time_to_diagnosis'] <= 730)).astype(int)\n",
    "hr_threshold = 160\n",
    "hr_col = 'hr_over_' + str(hr_threshold)\n",
    "test_split_df[hr_col] = (test_split_df['max_hr_within_365_days'] > hr_threshold).astype(int)\n",
    "\n",
    "## Rename for ease of reading\n",
    "test_split_df['binary_NoPCPEncounter'] = test_split_df['PCPvisits_bin'] == 0\n",
    "test_split_df['binary_NoCAREncounter'] = test_split_df['CARvisits_bin'] == 0\n",
    "test_split_df['binary_NoOTHEncounter'] = test_split_df['OTHvisits_bin'] == 0\n",
    "test_split_df['binary_prim_language'] = test_split_df['PrimLangDSC'].map(lambda x: x if x == 'English' else 'Non-English')\n",
    "\n",
    "risk_score_col_name = preprocessing_params['risk_score_col_name']\n",
    "covariate_names =  [ 'binary_Female', \n",
    "                     'PatientAge_years_01',\n",
    "                     'binary_Black or African American', \n",
    "                     'binary_Hispanic or Latino',\n",
    "                     'binary_Asian', 'binary_Other',\n",
    "                     'binary_Non-English',\n",
    "                     'binary_Medicare', 'binary_Medicaid',\n",
    "                     'binary_NoPCPEncounter',\n",
    "                     'binary_NoCAREncounter',\n",
    "                     'binary_NoOTHEncounter',\n",
    "                     risk_score_col_name ]\n",
    "\n",
    "\n",
    "# # # Kitchen sink\n",
    "# covariate_names = dem_feat_names + ecg_feat_names + ['binary_NoPCPEncounter',  'binary_NoCAREncounter',  'binary_NoOTHEncounter']\n",
    "\n",
    "# # Demographics alone\n",
    "# # covariate_names = dem_feat_names \n",
    "# covariate_names = ['binary_Black or African American', \n",
    "#                          'binary_Hispanic or Latino',\n",
    "#                          'binary_Asian', 'binary_Other']\n",
    "\n",
    "rar_df = test_split_df\n",
    "rar_model = preprocessing_params['rar_model']\n",
    "\n",
    "target_cols = ['diagnosis_in_charts', 'label', hr_col, 'stroke_within_year', 'diagnosis_within_3yr', 'diagnosis_within_2yr' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Outcome rates conditional on risk for AF diagnosis and ECG with HR > 160 (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import colors, prettify_col_name\n",
    "outcome_cols = ['diagnosis_in_charts', 'label', hr_col, 'stroke_within_year']\n",
    "outcome_cols = ['diagnosis_in_charts', hr_col]\n",
    "axis_params = [('PatientRaceFinal', ('White', 'Black or African American', 'Hispanic or Latino', 'Asian')),\n",
    "               ('instype_final', ('Commercial', 'Medicare', 'Medicaid')),\n",
    "                ('binary_prim_language', ('English', 'Non-English')),]\n",
    "\n",
    "n_rows = len(outcome_cols)\n",
    "n_cols = len(axis_params)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(9, 6), sharey=True, sharex=True, dpi=150)\n",
    "test_split_df['lpm_adjusted_risk_score_rank'] = test_split_df['lpm_adjusted_risk_score'].rank(method='first')\n",
    "test_split_df['quintile'] = pd.qcut(test_split_df['lpm_adjusted_risk_score_rank'], q=5, labels=False)\n",
    "test_split_df['quintile'] = test_split_df['quintile'] + 1\n",
    "\n",
    "\n",
    "for j, (group_col, group_vals) in enumerate(axis_params):\n",
    "    for i, outcome_col in enumerate(outcome_cols):\n",
    "        ax = axs[i, j]\n",
    "        plt.sca(ax)\n",
    "        \n",
    "        plot_df = test_split_df[test_split_df[group_col].isin(group_vals)]\n",
    "        plot_df['outcome_pct'] = plot_df[outcome_col]*100\n",
    "        sns.lineplot(\n",
    "            data=plot_df, \n",
    "            x=\"quintile\", \n",
    "            y='outcome_pct', \n",
    "            hue=group_col, \n",
    "            hue_order=group_vals,\n",
    "            palette=[colors[group_val] for group_val in group_vals], \n",
    "            ax=ax,\n",
    "            marker=\"o\",\n",
    "            errorbar=\"ci\",\n",
    "            err_style=\"bars\"\n",
    "\n",
    "        )\n",
    "        ax.set_title(\"\")\n",
    "        if i == 0:\n",
    "            ax.set_title(prettify_col_name(group_col))\n",
    "        ax.set_xlabel(\"Risk Quintile\")\n",
    "        ax.set_ylabel(\"HR > 160\\nOutcome (%)\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Diagnosis\\nOutcome (%)\")\n",
    "        plt.legend(title=prettify_col_name(group_col), fontsize=8)\n",
    "        if i == 0:\n",
    "            plt.legend().remove()\n",
    "        plt.grid(color='gray', alpha=0.4, linestyle='--')\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Outcome rates conditional on risk for AF diagnosis, ECG with HR > 180, AF ECG within 90 days, Stroke (Figure S4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import colors, prettify_col_name\n",
    "\n",
    "outcome_cols = ['diagnosis_in_charts', hr_col, 'label', 'stroke_within_year']\n",
    "axis_params = [('PatientRaceFinal', ('White', 'Black or African American', 'Hispanic or Latino', 'Asian')),\n",
    "               ('instype_final', ('Commercial', 'Medicare', 'Medicaid')),\n",
    "                ('binary_prim_language', ('English', 'Non-English')),]\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(10, 10), sharey=True, sharex=True, dpi=150)\n",
    "test_split_df['lpm_adjusted_risk_score_rank'] = test_split_df['lpm_adjusted_risk_score'].rank(method='first')\n",
    "test_split_df['quintile'] = pd.qcut(test_split_df['lpm_adjusted_risk_score_rank'], q=5, labels=False)\n",
    "test_split_df['quintile'] = test_split_df['quintile'] + 1\n",
    "\n",
    "rates_to_report = {}\n",
    "\n",
    "for j, (group_col, group_vals) in enumerate(axis_params):\n",
    "    for i, outcome_col in enumerate(outcome_cols):\n",
    "        if outcome_col not in rates_to_report:\n",
    "            rates_to_report[outcome_col] = {}\n",
    "        ax = axs[i, j]\n",
    "        plt.sca(ax)\n",
    "        \n",
    "        plot_df = test_split_df[test_split_df[group_col].isin(group_vals)]\n",
    "        plot_df[outcome_col] = plot_df[outcome_col]*100\n",
    "        sns.lineplot(\n",
    "            data=test_split_df[test_split_df[group_col].isin(group_vals)], \n",
    "            x=\"quintile\", \n",
    "            y=outcome_col, \n",
    "            hue=group_col, \n",
    "            hue_order=group_vals,\n",
    "            palette=[colors[group_val] for group_val in group_vals], \n",
    "            ax=ax,\n",
    "            marker=\"o\",\n",
    "            errorbar=\"ci\",\n",
    "            err_style=\"bars\"\n",
    "\n",
    "        )\n",
    "        ax.set_title(\"\")\n",
    "        if j == 1:\n",
    "            ax.set_title(prettify_col_name(outcome_col))\n",
    "        ax.set_xlabel(\"Risk Quintile\")\n",
    "        ax.set_ylabel(\"Outcome (%)\")\n",
    "        plt.legend(title=prettify_col_name(group_col), fontsize=8)\n",
    "        if i != 1:\n",
    "            plt.legend().remove()\n",
    "        plt.grid(color='gray', alpha=0.4, linestyle='--')\n",
    "\n",
    "        if outcome_col in ['diagnosis_within_year', 'diagnosis_in_charts', 'label', hr_col, 'stroke_within_year']:\n",
    "            ref_group = test_split_df[test_split_df[group_col] == group_vals[0]]\n",
    "            ref_group_rate = ref_group[ref_group['quintile'] == 5][outcome_col].mean()\n",
    "            # print(f\"{group_vals[0]} diagnosis rate in highest quintile: \", ref_group_rate)\n",
    "            rates_to_report[outcome_col][group_vals[0]] = {}\n",
    "            rates_to_report[outcome_col][group_vals[0]]['rate'] = np.round(ref_group_rate*100, 1)\n",
    "            rates_to_report[outcome_col][group_vals[0]]['n'] = len(ref_group)\n",
    "\n",
    "            for group_val in group_vals[1:]:\n",
    "                rates_to_report[outcome_col][group_val] = {}\n",
    "                group = test_split_df[test_split_df[group_col] == group_val]\n",
    "                group = group[group['quintile'] == 5]\n",
    "                # print(f\"{group_val} {outcome_col} rate in highest quintile: \", group[outcome_col].mean())\n",
    "                rates_to_report[outcome_col][group_val]['rate'] = np.round(group[outcome_col].mean()*100, 1)\n",
    "                rates_to_report[outcome_col][group_val]['n'] = len(group)\n",
    "            # print()\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Define confidence interval function for binary proportions\n",
    "def proportion_confint(successes, n, alpha=0.05):\n",
    "    p = successes / n\n",
    "    z = norm.ppf(1 - alpha / 2)\n",
    "    se = np.sqrt(p * (1 - p) / n)\n",
    "    return p, p - z * se, p + z * se\n",
    "\n",
    "# Subset the data\n",
    "rate_comparison_df = test_split_df[\n",
    "    test_split_df['PatientRaceFinal'].isin(['White', 'Black or African American'])\n",
    "]\n",
    "rate_comparison_df = rate_comparison_df[rate_comparison_df['quintile'] == 5]\n",
    "\n",
    "rates_with_cis_to_report = {}\n",
    "\n",
    "# Groupby aggregation with counts and sums\n",
    "summary = rate_comparison_df.groupby('PatientRaceFinal').agg(\n",
    "    n=('label', 'count'),\n",
    "    label_sum=('label', 'sum'),\n",
    "    diagnosis_in_charts_sum=('diagnosis_in_charts', 'sum'),\n",
    "    hr_over_160_sum=(hr_col, 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Apply CI calculation\n",
    "for col in ['label', 'diagnosis_in_charts', hr_col]:\n",
    "    summary[[f'{col}_mean', f'{col}_ci_lower', f'{col}_ci_upper']] = summary.apply(\n",
    "        lambda row: proportion_confint(row[f'{col}_sum'], row['n']), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "# Optional: format for display\n",
    "display_cols = ['PatientRaceFinal']\n",
    "for col in ['label', 'diagnosis_in_charts', hr_col]:\n",
    "    summary[f'{col}_with_ci'] = summary.apply(\n",
    "        lambda row: f\"{row[f'{col}_mean']:.3f} ({row[f'{col}_ci_lower']:.3f}, {row[f'{col}_ci_upper']:.3f})\",\n",
    "        axis=1\n",
    "    )\n",
    "    display_cols.append(f'{col}_with_ci')\n",
    "\n",
    "# Final output table\n",
    "summary[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Output variables referenced in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_str_with_mean_ci(mean_ci_str):\n",
    "    mean, lci, uci = mean_ci_str.split(' ')\n",
    "    return np.round(float(mean)*100,1), np.round(float(lci[1:-1])*100, 1), np.round(float(uci[:-1])*100, 1)\n",
    "meanAFWhite, lciAFWhite, uciAFWhite= parse_str_with_mean_ci(summary[summary['PatientRaceFinal']  == 'White']['label_with_ci'].iloc[0])\n",
    "meanAFBlack, lciAFBlack, uciAFBlack= parse_str_with_mean_ci(summary[summary['PatientRaceFinal']  == 'Black or African American']['label_with_ci'].iloc[0])\n",
    "meanRVRWhite, lciRVRWhite, uciRVWhite= parse_str_with_mean_ci(summary[summary['PatientRaceFinal']  == 'White']['hr_over_160_with_ci'].iloc[0])\n",
    "meanRVRBlack, lciRVRBlack, uciRVRBlack= parse_str_with_mean_ci(summary[summary['PatientRaceFinal']  == 'Black or African American']['hr_over_160_with_ci'].iloc[0])\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\rateDiagWhiteHighest}}{ {rates_to_report['diagnosis_in_charts']['White']['rate']}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateDiagBlackHighest}}{ {rates_to_report['diagnosis_in_charts']['Black or African American']['rate']}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateDiagHispHighest}}{ {rates_to_report['diagnosis_in_charts']['Hispanic or Latino']['rate']}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateDiagAsianHighest}}{ {rates_to_report['diagnosis_in_charts']['Asian']['rate']}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\rateDiagCommercialHighest}}{ {rates_to_report['diagnosis_in_charts']['Commercial']['rate']}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateDiagMedicaidHighest}}{ {rates_to_report['diagnosis_in_charts']['Medicaid']['rate']}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\rateDiagEngHighest}}{ {rates_to_report['diagnosis_in_charts']['English']['rate']}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateDiagNonEngHighest}}{ {rates_to_report['diagnosis_in_charts']['Non-English']['rate']}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\rateMeanAFWhiteHighest}}{ {meanAFWhite}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateLCIAFWhiteHighest}}{ {lciAFWhite}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateUCIAFWhiteHighest}}{ {uciAFWhite}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\rateMeanAFBlackHighest}}{ {meanAFBlack}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateLCIAFBlackHighest}}{ {lciAFBlack}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateUCIAFBlackHighest}}{ {uciAFBlack}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\rateMeanRVRWhiteHighest}}{ {meanRVRWhite}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateLCIRVRWhiteHighest}}{ {lciRVRWhite}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateUCIRVRWhiteHighest}}{ {uciRVWhite}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\rateMeanRVRBlackHighest}}{ {meanRVRBlack}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateLCIRVRBlackHighest}}{ {lciRVRBlack}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rateUCIRVRBlackHighest}}{ {uciRVRBlack}}\")\n",
    "\n",
    "highest_quintile_AF_ECG_rate = np.round(test_split_df[test_split_df['quintile'] == 5]['label'].mean()*100, 1)\n",
    "lowest_quintile_AF_ECG_rate = np.round(test_split_df[test_split_df['quintile'] == 1]['label'].mean()*100, 1)\n",
    "highest_quintile_AF_ECG_rate, lowest_quintile_AF_ECG_rate\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\highestQuintileRate}}{ {highest_quintile_AF_ECG_rate}}\")\n",
    "print(f\"\\\\newcommand{{\\\\lowestQuintileRate}}{ {lowest_quintile_AF_ECG_rate}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run p-value tests to compare diagnosis rate in highest \n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "col_name = 'diagnosis_in_charts'\n",
    "# col_name = 'diagnosis_within_year'\n",
    "# Example inputs (replace with your actual numbers)\n",
    "nobs_a = rates_to_report[col_name]['White']['n']  \n",
    "count_a = int(rates_to_report[col_name]['White']['rate']*nobs_a/100)   # number of diagnoses in group B\n",
    "\n",
    "for group in ['Black or African American', 'Hispanic or Latino', 'Asian']:\n",
    "\n",
    "    \n",
    "\n",
    "    nobs_b = rates_to_report[col_name][group]['n']   # number of diagnoses in group B\n",
    "    count_b = int(rates_to_report[col_name][group]['rate']*nobs_b/100)    # group size for B\n",
    "\n",
    "    # Run two-proportion z-test\n",
    "    counts = [count_a, count_b]\n",
    "    nobs = [nobs_a, nobs_b]\n",
    "    print(counts, nobs, [count_a/nobs_a, count_b/nobs_b])\n",
    "    stat, pval = proportions_ztest(count=counts, nobs=nobs, alternative='two-sided')\n",
    "\n",
    "    print(f\"{group} P-value: {pval:.4f}\")\n",
    "\n",
    "nobs_a = rates_to_report[col_name]['Commercial']['n']  \n",
    "count_a = int(rates_to_report[col_name]['Commercial']['rate']*nobs_a/100)   # number of diagnoses in group B\n",
    "\n",
    "for group in ['Medicare', 'Medicaid']:\n",
    "\n",
    "    \n",
    "\n",
    "    nobs_b = rates_to_report[col_name][group]['n']   # number of diagnoses in group B\n",
    "    count_b = int(rates_to_report[col_name][group]['rate']*nobs_b/100)    # group size for B\n",
    "\n",
    "    # Run two-proportion z-test\n",
    "    counts = [count_a, count_b]\n",
    "    nobs = [nobs_a, nobs_b]\n",
    "    print(counts, nobs, [count_a/nobs_a, count_b/nobs_b])\n",
    "    stat, pval = proportions_ztest(count=counts, nobs=nobs, alternative='two-sided')\n",
    "\n",
    "    print(f\"{group} P-value: {pval:.4f}\")\n",
    "\n",
    "nobs_a = rates_to_report[col_name]['English']['n']  \n",
    "count_a = int(rates_to_report[col_name]['English']['rate']*nobs_a/100)   # number of diagnoses in group B\n",
    "\n",
    "for group in ['Non-English']:\n",
    "\n",
    "    \n",
    "\n",
    "    nobs_b = rates_to_report[col_name][group]['n']   # number of diagnoses in group B\n",
    "    count_b = int(rates_to_report[col_name][group]['rate']*nobs_b/100)    # group size for B\n",
    "\n",
    "    # Run two-proportion z-test\n",
    "    counts = [count_a, count_b]\n",
    "    nobs = [nobs_a, nobs_b]\n",
    "    print(counts, nobs, [count_a/nobs_a, count_b/nobs_b])\n",
    "    stat, pval = proportions_ztest(count=counts, nobs=nobs, alternative='two-sided')\n",
    "\n",
    "    print(f\"{group} P-value: {pval:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5 Compute RAR coefficients for each outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import deterministic_dict_hash\n",
    "controls = ['binary_Female', \n",
    "            'PatientAge_years_01',\n",
    "            'binary_NoPCPEncounter',\n",
    "            'binary_NoCAREncounter',\n",
    "            'binary_NoOTHEncounter',\n",
    "            risk_score_col_name\n",
    "            ]\n",
    "\n",
    "if preprocessing_params['rar_covariates'] == 'demographics_joint':\n",
    "    demographic_cols = [['binary_Black or African American', 'binary_Hispanic or Latino', 'binary_Asian', 'binary_Other', \n",
    "                        'binary_Medicare', 'binary_Medicaid', 'binary_Non-English']]\n",
    "else:\n",
    "    demographic_cols = [['binary_Black or African American', 'binary_Hispanic or Latino', 'binary_Asian', 'binary_Other'],\n",
    "                        ['binary_Medicare', 'binary_Medicaid'],\n",
    "                        ['binary_Non-English']]\n",
    "event_rates = test_split_df[test_split_df['PatientRaceFinal'] == 'White'][target_cols].mean()\n",
    "event_rate_map = dict(zip(event_rates.index, event_rates.values))\n",
    "normalize_by_event_rate = False\n",
    "\n",
    "all_CI_df = []\n",
    "for target_col in target_cols:\n",
    "    for demographic_col_names  in demographic_cols:\n",
    "        df = rar_df.copy()\n",
    "        df = df[controls + demographic_col_names + [target_col]].dropna()\n",
    "\n",
    "        # Separate independent variables (X) and dependent variable (y)\n",
    "        X = df.drop(target_col, axis=1)  # Independent variables\n",
    "        X = X[controls +  demographic_col_names].astype(float)\n",
    "\n",
    "        y = df[target_col].astype(int)  # Dependent variable\n",
    "\n",
    "        # Add a constant to the model (intercept)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # Build the model\n",
    "        if rar_model == 'Logit':\n",
    "            model = sm.Logit(y, X)\n",
    "        elif rar_model == 'OLS':\n",
    "            model = sm.OLS(y, X)\n",
    "\n",
    "\n",
    "        # Fit the model\n",
    "        results = model.fit(cov='HC3')\n",
    "        CI_df = convert_results_to_CI_df(results)\n",
    "        CI_df = CI_df.drop('const')\n",
    "        if risk_score_col_name in CI_df.columns:\n",
    "            CI_df = CI_df.drop(risk_score_col_name)\n",
    "\n",
    "        for col_name in controls:\n",
    "            if col_name in CI_df.columns:\n",
    "                CI_df = CI_df.drop(col_name)\n",
    "\n",
    "        for col_name in ['Estimate', 'Lower bound', 'Upper bound']:\n",
    "            if rar_model == 'OLS':\n",
    "                CI_df[col_name] = CI_df[col_name] * 100\n",
    "        if normalize_by_event_rate:\n",
    "            for col_name in ['Estimate', 'Lower bound', 'Upper bound']:\n",
    "                CI_df[col_name] = CI_df[col_name] / event_rate_map[target_col]\n",
    "            \n",
    "\n",
    "        CI_df['target_col'] = target_col\n",
    "        all_CI_df.append(CI_df)\n",
    "\n",
    "all_CI_df = pd.concat(all_CI_df)\n",
    "all_CI_df['target_col'] = all_CI_df['target_col'].map({'diagnosis_in_charts': 'diag', 'label': 'ecg', 'hr_over_160': 'rvr', 'stroke': 'stroke', 'diagnosis_within_year': 'diag_1yr',\n",
    "                                                       'spec_vis': 'spec', 'trt_rate': 'trt', 'trt_rhythm': 'trt', 'stroke_within_year': 'stroke_within_year',\n",
    "                                                       'anticoag_treatment': 'anticoag_treatment',\n",
    "                                                        'diagnosis_within_3yr': 'diag_3yr', 'diagnosis_within_2yr': 'diag_2yr' })   \n",
    "\n",
    "all_CI_df.to_csv('./outputs_est_coeffs/' + deterministic_dict_hash(preprocessing_params))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6 Plot RAR coeffs (Figure 4, Figure S5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_names = ['binary_Black or African American',\n",
    "                'binary_Hispanic or Latino', 'binary_Asian', 'binary_Medicaid', 'binary_Medicare', 'binary_Non-English']\n",
    "\n",
    "panel_names_order = ['diag', 'rvr'] # Figure 4\n",
    "panel_names_order = ['diag', 'rvr', 'ecg', 'stroke_within_year'] # Figure S5\n",
    "\n",
    "n_rows = 1\n",
    "fig, axs = plt.subplots(n_rows, len(panel_names_order), figsize=(15, 4 ), sharex=True, sharey=True, dpi=100)\n",
    "\n",
    "\n",
    "for i, panel_name in enumerate(panel_names_order):\n",
    "    ax = axs[i]\n",
    "    plt.sca(ax)\n",
    "    plot_CI_df = all_CI_df[all_CI_df['target_col'] == panel_name].copy()\n",
    "    plot_CI_df = plot_CI_df[plot_CI_df.index.isin(covariate_names)]\n",
    "    plot_CI_df['cleaned_group_name'] = plot_CI_df.index.map(lambda x: x.split('_')[1])\n",
    "    plot_CI_df.set_index('cleaned_group_name', inplace=True)\n",
    "    dependent_vars = [panel_name]\n",
    "    ax = plot_CIs_covariates(plot_CI_df[['Estimate', 'Lower bound', 'Upper bound']], covariate_names=[x.split('_')[1] for x in covariate_names],\n",
    "                             show=False, ax=ax, ylabel_size=15, xlabel_size=15, horizontal_lines=False)\n",
    "    \n",
    " \n",
    "    plt.title(table_name_target_col_map[panel_name],  fontsize=15)\n",
    "    plt.grid(color='lightgray', alpha=.5)\n",
    "\n",
    "    plt.xlabel(\"Effect Size\", fontsize=15)\n",
    "    if rar_model == 'OLS':\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: f\"{y:.0f}%\"))\n",
    "    \n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data['PrimLangDSC'] = ecg_data['PrimLangDSC'].apply(lambda x: 'English' if x == 'English' else 'Non-English')\n",
    "\n",
    "demographic_to_n = {}\n",
    "for demographic_group in ['PatientRaceFinal', 'instype_final', 'PrimLangDSC']:\n",
    "    gg = ecg_data[demographic_group].value_counts()\n",
    "    for group_value, count in gg.items():\n",
    "        print(group_value)\n",
    "        demographic_to_n[group_value] = count\n",
    "\n",
    "\n",
    "demographic_to_n_diag = {}\n",
    "for demographic_group in ['PatientRaceFinal', 'instype_final', 'PrimLangDSC']:\n",
    "    gg = ecg_data.groupby(demographic_group)['diagnosis_in_charts'].sum()\n",
    "    for group_value, count in gg.items():\n",
    "        demographic_to_n_diag[group_value] = count\n",
    "        \n",
    "rar_df['binary_White'] = (rar_df['PatientRaceFinal'] == 'White').astype(int)\n",
    "rar_df['binary_Commercial'] = (rar_df['instype_final'] == 'Commercial').astype(int)\n",
    "rar_df['binary_English'] = (rar_df['PrimLangDSC'] == 'English').astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7 Output prevalence analysis (Table 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_cols = [(['binary_Black or African American', 'binary_Hispanic or Latino', 'binary_Asian'], 'White', 'Race'),\n",
    "                    (['binary_Medicare', 'binary_Medicaid'], 'Commercial', 'Insurance'),\n",
    "                    (['binary_Non-English'], 'English', 'Primary Language'),\n",
    "                         ]\n",
    "\n",
    "all_demographics = ['binary_Black or African American',\n",
    "                'binary_Hispanic or Latino', 'binary_Asian', 'binary_Medicaid', 'binary_Medicare', 'binary_Non-English']\n",
    "\n",
    "demographic_cols = [(['binary_White', 'binary_Hispanic or Latino', 'binary_Asian'], 'Black or African American', 'Race'),\n",
    "                    (['binary_Medicare', 'binary_Medicaid'], 'Commercial', 'Insurance'),\n",
    "                    (['binary_Non-English'], 'English', 'Primary Language'),\n",
    "                         ]\n",
    "\n",
    "all_demographics = ['binary_White',\n",
    "                'binary_Hispanic or Latino', 'binary_Asian', 'binary_Medicaid', 'binary_Medicare', 'binary_Non-English']\n",
    "prevalence_df = []\n",
    "n_decimals = 1\n",
    "for demographic_col_names, default_group_name, category_name in demographic_cols:\n",
    "    for group_name in demographic_col_names:\n",
    "\n",
    "        # When other_demographics is empty, results correspond to the following model:\n",
    "        # diag ~ race + risk_score + controls \n",
    "        # When other_demographics is not empty, we fit the following model:\n",
    "        # diag ~ race + insurance + primary language + risk_score + controls\n",
    "        # controls contains age, sex, access to care indicators, and the risk score\n",
    "        other_demographics = [x for x in all_demographics if x not in demographic_col_names]\n",
    "        # other_demographics = []\n",
    "\n",
    "        target_col = 'diagnosis_in_charts'\n",
    "        df = rar_df.copy()\n",
    "        df = df[controls + demographic_col_names + other_demographics + [target_col]].dropna()\n",
    "\n",
    "        X = df[controls + demographic_col_names + other_demographics].astype(float)\n",
    "        y = df[target_col].astype(int)  \n",
    "\n",
    "        # Add a constant to the model (intercept)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # Fit model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "        # test_split_df is equivalent to study split we refer to in paper.\n",
    "        orig_rate = test_split_df[test_split_df[group_name] == 1]['diagnosis_in_charts'].mean()\n",
    "        group_patients = X[X[group_name] == 1].copy()\n",
    "\n",
    "        # demographic_col_names refers to columns pertaining to that specific demographic;\n",
    "        # for example, when computing race prevalences, the insurance and primary language features within a group\n",
    "        # are untouched\n",
    "        for col in demographic_col_names:\n",
    "            group_patients[col] = 0\n",
    "\n",
    "        # Take average prediction for diagnosis across all patients within group,\n",
    "        # This measures, for example, the rate of diagnosis if all black patients were white using the estimated coefficients.\n",
    "        group_pred = model.predict(group_patients)\n",
    "        corrected_rate = group_pred.mean()\n",
    "        diff = corrected_rate - orig_rate\n",
    "\n",
    "        orig_rate = f'{np.round(orig_rate*100, n_decimals)}'\n",
    "        est_rate = f'{np.round(corrected_rate*100, n_decimals)}' \n",
    "        n_hidden_diags = int(diff*demographic_to_n[group_name.split('_')[-1]])\n",
    "        n_diag = demographic_to_n_diag[group_name.split('_')[-1]]\n",
    "        print(group_name, n_hidden_diags, n_diag)\n",
    "        change_rate = np.round(100*n_hidden_diags/n_diag, n_decimals)\n",
    "        prevalence_df.append({'Category': category_name, 'Group': group_name.split('_')[1], 'Obs. Prevalence': orig_rate + f' ({n_diag})', 'Est. Prevalence': est_rate + f' ({n_diag + n_hidden_diags})', \n",
    "                               'Change in Diagnosis': str(change_rate) + f' ({n_hidden_diags})'})\n",
    "    orig_rate = test_split_df[test_split_df[demographic_col_names].sum(axis=1) == 0]['diagnosis_in_charts'].mean()\n",
    "    n_diag = demographic_to_n_diag[default_group_name]\n",
    "    default_patients = X[X[demographic_col_names].sum(axis=1) == 0]\n",
    "    group_pred = model.predict(default_patients)\n",
    "    corrected_rate = group_pred.mean()\n",
    "    diff = corrected_rate - orig_rate\n",
    "    n_hidden_diags = int(diff*demographic_to_n[default_group_name])\n",
    "    orig_rate = f'{np.round(orig_rate*100, n_decimals)}'\n",
    "    est_rate = f'{np.round(corrected_rate*100, n_decimals)}' \n",
    "    change_rate = np.round(100*n_hidden_diags/n_diag, n_decimals)\n",
    "    prevalence_df.append({'Category': category_name, 'Group': default_group_name, 'Obs. Prevalence': orig_rate + f' ({n_diag})', \n",
    "                          'Est. Prevalence': est_rate + f' ({n_diag + n_hidden_diags})', \n",
    "                               'Change in Diagnosis': str(change_rate) + f' ({n_hidden_diags})'})\n",
    "    print()\n",
    "prevalence_df = pd.DataFrame(prevalence_df)\n",
    "\n",
    "print(prevalence_df.to_latex(index=False, column_format=\"lcc\", caption=\"Corrected Prevalence Rates by Group\", label=\"tab:prevalences\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for display name cleanup\n",
    "value_rename_map = {\n",
    "    'BlackorAfricanAmerican': 'Black/African American',\n",
    "    'HispanicorLatino': 'Hispanic/Latino'\n",
    "}\n",
    "\n",
    "# Start LaTeX table (requires \\usepackage{makecell} in LaTeX preamble)\n",
    "latex_rows = []\n",
    "latex_rows.append(r\"\\begin{tabular}{llccc}\")\n",
    "latex_rows.append(r\"\\toprule\")\n",
    "latex_rows.append(r\" & & \\makecell{\\textbf{Obs. Prevalence}} & \\makecell{\\textbf{Est. Prevalence}\\\\\\textbf{$\\Delta$ in Diagnosis Rate} \\\\\")\n",
    "latex_rows.append(r\" & & \\makecell{\\textbf{% (N)}} & \\makecell{\\textbf{% (N)}\\\\\\textbf{% (N)} \\\\\")\n",
    "latex_rows.append(r\"\\hline\")\n",
    "\n",
    "prev_category = None\n",
    "for _, row in prevalence_df.iterrows():\n",
    "    category = row['Category']\n",
    "    val = row['Group']\n",
    "    category = category.strip()\n",
    "    val = val.strip().replace(' ', '')\n",
    "\n",
    "    # Clean display values\n",
    "    val = value_rename_map.get(val, val)\n",
    "\n",
    "    # Only show category label the first time it appears\n",
    "    group_col = f\"\\\\textbf{{{category}}}\" if category != prev_category else \"\"\n",
    "    if category != prev_category:\n",
    "        latex_rows.append(\"\\midrule\")\n",
    "    prev_category = category\n",
    "\n",
    "    # Build LaTeX row\n",
    "    latex_row = f\"{group_col} & {val}  & {row['Obs. Prevalence']} & {row['Est. Prevalence']} & {row['Change in Diagnosis']} \\\\\\\\\"\n",
    "    latex_rows.append(latex_row)\n",
    "latex_rows.append(r\"\\bottomrule\")\n",
    "latex_rows.append(r\"\\end{tabular}\")\n",
    "\n",
    "# Combine and print\n",
    "latex_code = \"\\n\".join(latex_rows)\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Run supplementary analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 Compare to baselines (Figure S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dem_feat_names, ecg_feat_names\n",
    "\n",
    "risk_score_col_name = 'lpm_adjusted_risk_score'\n",
    "comparison_test_split_df = test_split_df.copy()\n",
    "dem_ecg_feat_names = dem_feat_names + ecg_feat_names\n",
    "comparison_test_split_df = comparison_test_split_df[dem_ecg_feat_names + ['label'] + [risk_score_col_name]]\n",
    "comparison_test_split_df = comparison_test_split_df.dropna()\n",
    "\n",
    "for i, (feat_set_name, feat_names) in enumerate([('dems', dem_feat_names),\n",
    "                    ('ecg', ecg_feat_names),\n",
    "                    ('dem_ecg_feats', dem_ecg_feat_names)]):\n",
    "\n",
    "    train_split_df = split_dfs[0]\n",
    "    train_X_Y = train_split_df[feat_names + ['label']].dropna()\n",
    "    train_X = train_X_Y[feat_names].values\n",
    "    train_Y = train_X_Y['label'].values\n",
    "\n",
    "    test_X_Y = comparison_test_split_df[feat_names + ['label']].dropna()\n",
    "    test_X = test_X_Y[feat_names].values\n",
    "    test_Y = test_X_Y['label'].values\n",
    "\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.transform(test_X)\n",
    "\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_X, train_Y)\n",
    "\n",
    "    lr_pred_probs = lr.predict_proba(test_X)[:,1]\n",
    "    comparison_test_split_df['lr_risk_score_' + feat_set_name] = lr_pred_probs\n",
    "    print(feat_set_name, roc_auc_score(test_Y, lr_pred_probs))\n",
    "\n",
    "\n",
    "# Plot AUC curves\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6), dpi=150)\n",
    "\n",
    "prettify_risk_score_col_name = {\n",
    "    'lpm_adjusted_risk_score': 'Our Model',\n",
    "    'lr_risk_score_dem_ecg_feats': 'LR + Demographics, ECG',\n",
    "    'lr_risk_score_ecg': 'LR + ECG',\n",
    "    'lr_risk_score_dems': 'LR + Demographics'\n",
    "}\n",
    "for i, risk_score_to_compare in enumerate(['lpm_adjusted_risk_score', 'lr_risk_score_dem_ecg_feats', \n",
    "                                            'lr_risk_score_ecg', 'lr_risk_score_dems']):\n",
    "\n",
    "    x = comparison_test_split_df[risk_score_to_compare]\n",
    "    y = comparison_test_split_df['label']\n",
    "    roc = ru.compute_roc(X=x, y=y, pos_label=1)\n",
    "\n",
    "    ru.plot_roc(roc,color=colors[risk_score_to_compare], \n",
    "                label=prettify_risk_score_col_name[risk_score_to_compare])\n",
    "    \n",
    "    plt.title('Comparison to Alternative Models')\n",
    "    if i!=0:\n",
    "        plt.ylabel(\"\")\n",
    "        \n",
    "    # Plot AUC curves\n",
    "fig = plt.figure(figsize=(6, 6), dpi=150)\n",
    "\n",
    "prettify_risk_score_col_name = {\n",
    "    'lpm_adjusted_risk_score': 'Our Model',\n",
    "    'lr_risk_score_dem_ecg_feats': 'LR + Demographics, ECG',\n",
    "    'lr_risk_score_ecg': 'LR + ECG',\n",
    "    'lr_risk_score_dems': 'LR + Demographics'\n",
    "}\n",
    "for i, risk_score_to_compare in enumerate(['lpm_adjusted_risk_score', 'lr_risk_score_dem_ecg_feats', \n",
    "                                            'lr_risk_score_ecg', 'lr_risk_score_dems']):\n",
    "\n",
    "    x = comparison_test_split_df[risk_score_to_compare]\n",
    "    y = comparison_test_split_df['label']\n",
    "    roc = ru.compute_roc(X=x, y=y, pos_label=1)\n",
    "\n",
    "    ru.plot_roc(roc,color=colors[risk_score_to_compare], \n",
    "                label=prettify_risk_score_col_name[risk_score_to_compare])\n",
    "    \n",
    "    plt.title('Comparison to Alternative Models')\n",
    "    if i!=0:\n",
    "        plt.ylabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 Plot distributions of estimated risk by demographic group (Figure S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_df['PrimLangDSC'] = test_split_df['PrimLangDSC'].apply(lambda x: 'English' if x == 'English' else 'Non-English')\n",
    "\n",
    "axis_params = [('PatientRaceFinal', ('White', 'Black or African American', 'Hispanic or Latino', 'Asian')),\n",
    "               ('instype_final', ('Commercial', 'Medicare', 'Medicaid')),\n",
    "                ('PrimLangDSC', ('English', 'Non-English')),\n",
    "                ]\n",
    "\n",
    "\n",
    "# Risk score column name\n",
    "risk_score_col_name = 'lpm_adjusted_risk_score'  # Update with the actual column name\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "for ax, (col_name, values) in zip(axes, axis_params):\n",
    "    # Filter the DataFrame to include only rows with the specified values\n",
    "    plt.sca(ax)\n",
    "    filtered_df = test_split_df[test_split_df[col_name].isin(values)]\n",
    "    filtered_df['log_' + risk_score_col_name] = np.log(filtered_df[risk_score_col_name])\n",
    "    # filtered_df = filtered_df[filtered_df['diagnosis_in_charts'] == 1]\n",
    "    # Plot the distribution of risk scores within each group\n",
    "    sns.kdeplot(\n",
    "        data=filtered_df,\n",
    "        x='log_' + risk_score_col_name,\n",
    "        hue=col_name,\n",
    "        ax=ax,\n",
    "        # stat = 'density',\n",
    "        # bins=50,\n",
    "        common_norm=False, \n",
    "        # cumulative=False,\n",
    "        legend=True,\n",
    "        bw_adjust=1,\n",
    "    )\n",
    "    ax.get_legend().set_title(prettify_col_name(col_name))\n",
    "    # Set axis labels and title\n",
    "    ax.set_title(f'Distribution of Risk by {prettify_col_name(col_name)}')\n",
    "    ax.set_xlabel(\"log(Predicted Risk of AF within 90 days)\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.grid(color='lightgray', linestyle='--', linewidth=0.5, zorder=-100)\n",
    "\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 Sensitivity analyses (Figure S6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = './outputs_est_coeffs/'\n",
    "\n",
    "diag_config = {'max_pred_gap': 90, 'selection_criteria': 'va', 'include_single_ecgs': True, 'mini': False, 'one_ecg_per_patient': 'last_two_ecgs', 'loss': 'CE', 'test_set_size': 0.4,\n",
    "                'rar_model': 'OLS', 'risk_score_col_name': 'lpm_adjusted_risk_score','rar_covariates': 'demographics_alone'}\n",
    "# diag_config = {'max_pred_gap': 90, 'selection_criteria': 'va', 'include_single_ecgs': True, 'mini': False, 'one_ecg_per_patient': 'last', 'loss': 'CE', 'test_set_size': 0.4,\n",
    "#                 'rar_model': 'OLS', 'risk_score_col_name': 'lpm_adjusted_risk_score','rar_covariates': 'demographics_alone'}\n",
    "# # Filter to the alternate diagnosis outcomes (2 year and 3 year)\n",
    "\n",
    "\n",
    "df = pd.read_csv(prefix + '/' + deterministic_dict_hash(diag_config))\n",
    "df = df[df['target_col'] == 'diag']\n",
    "df[df['Unnamed: 0'].isin(all_demographics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis\n",
    "\n",
    "# Read in coefficients from two different dataset\n",
    "all_configs = [('Patients with confirmed negative', {'max_pred_gap': 90, 'selection_criteria': 'va', 'include_single_ecgs': True, 'mini': False, 'one_ecg_per_patient': 'last_two_ecgs', 'loss': 'CE', 'test_set_size': 0.4,\n",
    "                'rar_model': 'OLS', 'risk_score_col_name': 'lpm_adjusted_risk_score','rar_covariates': 'demographics_alone'}), # Restrict dataset to patients with two ECGs. done.\n",
    "                ('White patients', {'max_pred_gap': 90, 'selection_criteria': 'va', 'include_single_ecgs': True, 'mini': False, 'one_ecg_per_patient': 'last_white', 'loss': 'CE', 'test_set_size': 0.4,\n",
    "                'rar_model': 'OLS', 'risk_score_col_name': 'lpm_adjusted_risk_score','rar_covariates': 'demographics_alone'}), # Restrict train + calibration dataset to white patients.\n",
    "                ('Joint regression', {'max_pred_gap': 90, 'selection_criteria': 'va', 'include_single_ecgs': True, 'mini': False, 'one_ecg_per_patient': 'last', 'loss': 'CE', 'test_set_size': 0.4,\n",
    "                'rar_model': 'OLS', 'risk_score_col_name': 'lpm_adjusted_risk_score','rar_covariates': 'demographics_joint'}), # Regress on all demographics at once, not individually\n",
    "\t\t\t\t('Main', {'max_pred_gap': 90, 'selection_criteria': 'va', 'include_single_ecgs': True, 'mini': False, 'one_ecg_per_patient': 'last', 'loss': 'CE', 'test_set_size': 0.4,\n",
    "                'rar_model': 'OLS', 'risk_score_col_name': 'lpm_adjusted_risk_score','rar_covariates': 'demographics_alone'}) # Main text results\n",
    "]\n",
    "\n",
    "coeffs_across_sensitivity_analyses = []\n",
    "prefix = './outputs/est_coeffs/'\n",
    "for config_name, config in all_configs:\n",
    "\tdf = pd.read_csv(prefix + '/' + deterministic_dict_hash(config))\n",
    "\tdf = df[df['target_col'] == 'diag']\n",
    "\tdf['config_name'] = config_name\n",
    "\tcoeffs_across_sensitivity_analyses.append(df)\n",
    "coeffs_across_sensitivity_analyses = pd.concat(coeffs_across_sensitivity_analyses)\n",
    "\n",
    "diag_config = {'max_pred_gap': 90, 'selection_criteria': 'va', 'include_single_ecgs': True, 'mini': False, 'one_ecg_per_patient': 'last', 'loss': 'CE', 'test_set_size': 0.4,\n",
    "                'rar_model': 'OLS', 'risk_score_col_name': 'lpm_adjusted_risk_score','rar_covariates': 'demographics_alone'}\n",
    "df = pd.read_csv(prefix + '/' + deterministic_dict_hash(diag_config))\n",
    "df = df[df['target_col'].isin(['diag_2yr', 'diag_3yr'])]\n",
    "\n",
    "df.rename(columns={'target_col': 'config_name'}, inplace=True)\n",
    "df['config_name'] = df['config_name'].map(lambda x: 'Diagnosis within 2 years' if x == 'diag_2yr' else 'Diagnosis within 3 years')\n",
    "\n",
    "coeffs_across_sensitivity_analyses = pd.concat([coeffs_across_sensitivity_analyses, df], axis=0)\n",
    "coeffs_across_sensitivity_analyses.rename(columns={'Unnamed: 0': 'Group'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covariate_names = ['binary_Black or African American',\n",
    "                'binary_Hispanic or Latino', 'binary_Asian', 'binary_Medicaid', 'binary_Medicare', 'binary_Non-English']\n",
    "\n",
    "config_names = ['Main', 'Patients with confirmed negative', 'White patients', 'Joint regression', 'Diagnosis within 2 years', 'Diagnosis within 3 years']\n",
    "panel_names_order = ['diag',  'ecg', 'rvr']\n",
    "n_rows = 2\n",
    "n_cols = int(len(covariate_names)/n_rows)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 5), sharex=True, sharey=True, dpi=200)\n",
    "\n",
    "\n",
    "for i, covariate_name in enumerate(covariate_names):\n",
    "    ax = axs[int(i/n_cols), i%n_cols]\n",
    "    plt.sca(ax)\n",
    "    plot_CI_df = coeffs_across_sensitivity_analyses[coeffs_across_sensitivity_analyses['Group'] == covariate_name]\n",
    "    plot_CI_df['cleaned_group_name'] = plot_CI_df['Group'].map(lambda x: x.split('_')[1])\n",
    "    plot_CI_df.set_index('config_name', inplace=True)\n",
    "    dependent_vars = [panel_name]\n",
    "    ax = plot_CIs_covariates(plot_CI_df[['Estimate', 'Lower bound', 'Upper bound']], covariate_names=config_names,\n",
    "                             show=False, ax=ax, ylabel_size=15, xlabel_size=15, horizontal_lines=False, color_CIs_by_significance=False)\n",
    "    \n",
    "    # _ = ax.set_xticks(ax.get_xticks()[::-1], [parameter.split('_')[-1] for parameter in list(covariate_names)], rotation=90)\n",
    "    _ = ax.set_xlim(-4.5, 3.5)\n",
    "    \n",
    "    plt.title(covariate_name.split('_')[1],  fontsize=15)\n",
    "    plt.grid(color='lightgray', alpha=.5)\n",
    "\n",
    "    plt.xlabel(\"\")\n",
    "    if int(i/n_cols) == 1:\n",
    "        plt.xlabel(\"Effect Size\", fontsize=15)\n",
    "    if rar_model == 'OLS':\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: f\"{y:.0f}%\"))\n",
    "    \n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Write out scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_include =  ['risk_score',\n",
    "                    'lr_adjusted_risk_score',\n",
    "                    'lpm_adjusted_risk_score',\n",
    "                    'Hosp',\n",
    "                    'LocationName',\n",
    "                    'binary_Male',\n",
    "                    'PrimLangDSC',\n",
    "                    'PatientAge_years',\n",
    "                    'PatientRaceFinal',\n",
    "                    'instype_final',\n",
    "                    'diagnosis_in_charts',\n",
    "                    'diagnosis_in_encounters',\n",
    "                    'diagnosis_in_problem_list',\n",
    "                    'anticoag_treatment',\n",
    "                    'spec_vis',\n",
    "                    'trt_rate',\n",
    "                    'trt_rhythm',\n",
    "                    'hr_over_160',\n",
    "                    'hospitalization',\n",
    "                    'mortality',\n",
    "                    'stroke',\n",
    "                    'PCPvisits_bin',\n",
    "                    'CARvisits_bin',\n",
    "                    'OTHvisits_bin',\n",
    "                    'afib_pos_ecg']\n",
    "test_split_df['afib_pos_ecg'] = test_split_df['label']\n",
    "test_split_df['Hosp'] = test_split_df['ecg_location']\n",
    "df_to_write_out = test_split_df[cols_to_include]\n",
    "df_to_write_out.to_csv('afib_risk_scores_outcomes_v7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Compute HR column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muse_cache_df = pd.concat([pd.read_csv('../outputs_intermediate/muse_cache_files/patient_mrn_to_file.csv', dtype='str'), \n",
    "                           pd.read_csv('../outputs_intermediate/muse_cache_files/patient_mrn_to_file_2020_2021_2022.csv', dtype='str')])\n",
    "muse_cache_df['date'] = pd.to_datetime(muse_cache_df[['year', 'month', 'day']])\n",
    "test_split_df['date'] = pd.to_datetime(test_split_df['date'])\n",
    "# For each UniqueID, determine the maximum heart-rate of an ECG that occured within some period of time\n",
    "\n",
    "from ecg_feature_names import  xml_file_to_extracted_features\n",
    "\n",
    "window = 365 # 1 year\n",
    "\n",
    "train_split_df = split_dfs[0]\n",
    "all_data = pd.concat([train_split_df, val_split_df, test_split_df])\n",
    "heldout_uniqueids = all_data['UniqueID'].unique()\n",
    "relevant_test_split = all_data[['UniqueID', 'date']]\n",
    "relevant_muse_cache = muse_cache_df[muse_cache_df['UniqueID'].isin(heldout_uniqueids)]\n",
    "\n",
    "# merge test_split_df and muse_cache_df on UniqueID; keep path and date \n",
    "# process each row, read in atrial rate and ventricular rate\n",
    "unique_id_to_max_hr = pd.merge(relevant_test_split, relevant_muse_cache, on='UniqueID', suffixes=('', '_other'))\n",
    "\n",
    "# Filter for ECGs after the initial date for each UniqueID, but within the time window\n",
    "unique_id_to_max_hr = unique_id_to_max_hr[unique_id_to_max_hr['date_other'] > unique_id_to_max_hr['date']]\n",
    "unique_id_to_max_hr = unique_id_to_max_hr[unique_id_to_max_hr['date_other'] <= unique_id_to_max_hr['date'] + pd.Timedelta(days=window)]\n",
    "len(unique_id_to_max_hr), unique_id_to_max_hr['UniqueID'].nunique(), test_split_df['UniqueID'].nunique()\n",
    "\n",
    "# Read in the heart rate for each ECG\n",
    "import ipyparallel as ipp\n",
    "# 21 minutes\n",
    "n_engines = 8\n",
    "cluster = ipp.Cluster(n=n_engines)\n",
    "cluster.start_cluster_sync()\n",
    "rc = cluster.connect_client_sync()\n",
    "rc.wait_for_engines(n_engines)\n",
    "dview = rc[:]\n",
    "\n",
    "all_paths = list(unique_id_to_max_hr['path'])\n",
    "xml_outputs = dview.map_sync(xml_file_to_extracted_features, all_paths)\n",
    "feature_dicts = [o for o in xml_outputs]\n",
    "feature_df = pd.DataFrame(feature_dicts)\n",
    "feature_df['path'] = all_paths\n",
    "\n",
    "unique_id_to_max_hr = pd.merge(unique_id_to_max_hr, feature_df, on='path')\n",
    "\n",
    "\n",
    "unique_id_to_max_hr.to_csv('../outputs_intermediate/unique_id_to_all_ecg_info_' + str(window)+ '_days.csv')\n",
    "\n",
    "unique_id_to_max_hr = pd.read_csv('../outputs_intermediate/unique_id_to_all_ecg_info_' + str(window)+ '_days.csv')\n",
    "unique_id_to_max_hr['UniqueID'].nunique(), len(unique_id_to_max_hr)\n",
    "unique_id_to_max_hr = unique_id_to_max_hr[['UniqueID', 'VentricularRate', 'AtrialRate']]\n",
    "unique_id_to_max_hr = unique_id_to_max_hr.groupby('UniqueID').max().reset_index()\n",
    "unique_id_to_max_hr.to_csv('../outputs_intermediate/unique_id_to_max_hr_' + str(window)+ '_days.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
