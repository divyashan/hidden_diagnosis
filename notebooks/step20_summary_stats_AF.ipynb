{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "import sys \n",
    "sys.path.append('../src/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "from utils import restrict_GPU_pytorch\n",
    "restrict_GPU_pytorch('0')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load extracted EDW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticoag_treatment = pd.read_csv('../extracted_EDW_data/trt_antic_ecglst.csv')\n",
    "anticoag_treatment['anticoag_treatment'] = ~anticoag_treatment['OrderDTS'].isna()\n",
    "anticoag_treatment = anticoag_treatment[['UniqueID', 'anticoag_treatment']]\n",
    "\n",
    "hospitalization = pd.read_csv('../extracted_EDW_data/hospital_vists.csv')\n",
    "hospitalization['HospitalAdmitDTS'] = pd.to_datetime(hospitalization['HospitalAdmitDTS'])\n",
    "hospitalization['ecg_date_lst'] = pd.to_datetime(hospitalization['ecg_date_lst'])\n",
    "hospitalization = hospitalization[hospitalization['HospitalAdmitDTS'] > hospitalization['ecg_date_lst']]\n",
    "hospitalization = hospitalization.sort_values('HospitalAdmitDTS')\n",
    "hospitalization = hospitalization.groupby('UniqueID').first().reset_index()\n",
    "\n",
    "# Filter for hospitalizations that occur after ecg_date_lst\n",
    "# Filter for earliest hospitalization\n",
    "specialist_visit = pd.read_csv('../extracted_EDW_data/specvis_lst.csv')\n",
    "specialist_visit = specialist_visit[['UniqueID', 'spec_vis']]\n",
    "\n",
    "insurance = pd.read_csv('../extracted_EDW_data/ins_ecglst.csv')\n",
    "insurance = insurance[['UniqueID', 'instype_final']]\n",
    "est_care = pd.read_csv('../extracted_EDW_data/estcare_ecglst.csv')\n",
    "est_care = est_care[['UniqueID', 'PCPvisits_bin', 'CARvisits_bin', 'OTHvisits_bin']]\n",
    "\n",
    "rate = pd.read_csv('../extracted_EDW_data/rate_ecglst.csv')\n",
    "rhythm = pd.read_csv('../extracted_EDW_data/rhythm_ecglst.csv')\n",
    "stroke = pd.read_csv('../extracted_EDW_data/stroke_ecglst.csv')\n",
    "\n",
    "rate = rate[['UniqueID', 'trt_rate']]\n",
    "rhythm = rhythm[['UniqueID', 'trt_rhythm']]\n",
    "stroke = stroke[['UniqueID', 'stroke']]\n",
    "\n",
    "exists_in_new_system = pd.read_csv('../extracted_EDW_data/Missing Patients EDW RZ.csv')\n",
    "unique_ids_in_new_system = exists_in_new_system[exists_in_new_system['patid_found'] == 1]['UniqueID'].unique()\n",
    "\n",
    "print(\"Treatment rate with rate control medications:\", rate['trt_rate'].mean())\n",
    "print(\"Treatment rate with rhythm control medications:\", rhythm['trt_rhythm'].mean()) \n",
    "print(\"Mean stroke:\", stroke['stroke'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load AF dataset and combine with EDW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import map_params_to_filename\n",
    "from ecg_datasets import ECGDemographicsDataset, ECGDataset\n",
    "from ecg_preprocessing_fs import create_name_dob_hash\n",
    "import pandas as pd \n",
    "\n",
    "outcome= 'afib'\n",
    "merge_with_EDW_vars = True\n",
    "preprocessing_params = {'max_pred_gap': 90, \n",
    "                        'selection_criteria': 'va', \n",
    "                        'include_single_ecgs': True, \n",
    "                        'mini': False}\n",
    "unique_id_col = 'UniqueID'\n",
    "\n",
    "\n",
    "ecg_data = pd.read_csv('./processed_data/processed_afib_' + map_params_to_filename(preprocessing_params) + '.csv')   \n",
    "ecg_data['PatientFirstName'].fillna('nan', inplace=True)\n",
    "ecg_data[unique_id_col] = create_name_dob_hash(ecg_data, 'PatientFirstName', 'PatientLastName', 'DateofBirth')\n",
    "\n",
    "print(\"# of (Patients, ECGs) before merging with map to PatientID: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "\n",
    "preprocessing_params['one_ecg_per_patient'] = 'last' # can be 'false', 'last', 'first', 'last_white', 'last_two_ecgs'\n",
    "preprocessing_params['loss'] = 'CE' # can be CE or Focal\n",
    "preprocessing_params['mini'] = False # This refers to subsampling the train set; the earlier setting refers to subsampling all of MUSE. Yes, this is not ideal..\n",
    "\n",
    "files_to_skip = ['/data/workspace/ekg_bwr_trunc_norm/003161595_08-05-2019_15-20-53_SCD10410491PA05082019152053.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/000122100_09-21-2019_01-02-14_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006363943_05-24-2018_18-00-22_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/010464725_05-24-2019_15-17-48_SCD12365371PA24052019151748.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/000951233_08-12-2019_15-28-53_SKJ14029684PA12082019152853.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/003145657_08-14-2019_11-01-13_SKJ13388441SA14082019110113.npy', \n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/035737121_08-13-2017_18-51-36_SKJ13408672SA13082017185136.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/003156598_11-22-2019_16-12-28_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-07-2019_08-28-52_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-11-2019_03-20-46_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-06-2019_23-35-11_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/001410670_04-19-2016_14-07-26_SCD06223397PA19042016140726.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002105344_08-10-2018_16-05-48_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002105344_08-10-2018_16-05-48_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002063599_01-26-2017_10-56-00_SCD07047035PA26012017105600.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/001410670_04-19-2016_14-07-26_SCD06223397PA19042016140726.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-06-2019_23-35-11_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/006619841_05-07-2019_01-21-07_none.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002120566_12-19-2018_08-39-41_SKJ14080390PA19122018083941.npy',\n",
    "                    '/data/workspace/ekg_bwr_trunc_norm/002021971_05-14-2016_09-49-49_none.npy'\n",
    "                    ]\n",
    "\n",
    "ecg_data = ecg_data[~ecg_data['path_to_bwr_trunc_norm_data'].isin(files_to_skip)]\n",
    "ecg_data['ecg_date'] = pd.to_datetime(ecg_data[['year', 'month', 'day']])\n",
    "ecg_data['DateofBirth'] = pd.to_datetime(ecg_data['DateofBirth'])\n",
    "ecg_data['PatientAge'] = ecg_data['ecg_date'] - ecg_data['DateofBirth']\n",
    "ecg_data['PatientAge_years'] = ecg_data['PatientAge'].dt.days / 365.2425\n",
    "ecg_data['PatientAge_years_01'] = ecg_data['PatientAge_years'] / 100\n",
    "ecg_data.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'PatientID', 'year', 'month', 'day', 'muse_mrn'], inplace=True)\n",
    "\n",
    "\n",
    "print(\"# of (Patients, ECGs) before merging with map to PatientID: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "muse_edw_map = pd.read_csv('muse_edw_map.csv', dtype='str')\n",
    "ecg_data = ecg_data[ecg_data['UniqueID'].isin(muse_edw_map['UniqueID'])]\n",
    "ecg_data = pd.merge(ecg_data, muse_edw_map[['UniqueID', 'PatientID']], on='UniqueID')\n",
    "print(\"# of (Patients, ECGs) after merging with map to PatientID: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "\n",
    "\n",
    "# Demographics based on Brianna's pull\n",
    "demographics_file = pd.read_csv('~/Documents/demographics_with_diagnosis_info.csv')\n",
    "demographics_file['earliest_diagnosis'] = pd.to_datetime(demographics_file['earliest_diagnosis'])\n",
    "\n",
    "pids = sorted(list(set(ecg_data[unique_id_col])))\n",
    "test_set_size = .4\n",
    "random_state = 0\n",
    "preprocessing_params['test_set_size'] = test_set_size\n",
    "# preprocessing_params['random_state'] = random_state\n",
    "train_ids, test_ids = train_test_split(pids, test_size=preprocessing_params['test_set_size'], random_state=random_state)\n",
    "val_ids, test_ids = train_test_split(test_ids, test_size=.5, random_state=random_state)\n",
    "\n",
    "if merge_with_EDW_vars:\n",
    "    # Merge with demographics\n",
    "    ecg_data = ecg_data[ecg_data['UniqueID'].isin(demographics_file['UniqueID'])]\n",
    "    print(\"# of (Patients, ECGs) after demographics merge: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "    ecg_data = pd.merge(ecg_data, demographics_file, on='UniqueID')\n",
    "\n",
    "\n",
    "    print(\"# of Patients in sample matched to some diagnosis: \", ecg_data['diagnosis_in_charts'].sum())\n",
    "    # Filter rows where diagnosis occurs AFTER ECG or there is no diagnosis in the charts\n",
    "    ecg_data = ecg_data[(~ecg_data['diagnosis_in_charts']) | (0 < (ecg_data['earliest_diagnosis']  - ecg_data['ecg_date']).dt.days)]\n",
    "    print(\"# of (Patients, ECGs) after filtering out established AFib diagnoses: \", ecg_data['UniqueID'].nunique(), len(ecg_data))\n",
    "    ecg_data['time_to_diagnosis'] = (ecg_data['earliest_diagnosis'] - ecg_data['ecg_date']).dt.days\n",
    "\n",
    "    # Add binary indicators for demographics\n",
    "    for race_val in ['White', 'Hispanic or Latino', 'Black or African American', 'Asian', 'Other',\n",
    "                     'Declined or Unavailable', 'Native American or Pacific Islander']:\n",
    "        ecg_data['binary_' + race_val] = ecg_data['PatientRaceFinal'] == race_val\n",
    "    ecg_data['binary_Male'] = ecg_data['SexDSC'] == 'Male'\n",
    "    ecg_data.drop(columns=['binary_Race_CAUCASIAN', 'binary_Race_HISPANIC', 'binary_Race_BLACK', 'binary_Race_HISPANIC'], inplace=True)\n",
    "\n",
    "    # Add indicators for downstream outcomes - treatment with anticoag for now\n",
    "    ecg_data = pd.merge(ecg_data, anticoag_treatment, on='UniqueID') # 13% of patients go on to have anticoag treatment \n",
    "    ecg_data['mortality'] = ~ecg_data['DeathDTS'].isna() # 12% of patients die\n",
    "    ecg_data['hospitalization'] = ecg_data['UniqueID'].isin(hospitalization['UniqueID']) \n",
    "    ecg_data = pd.merge(ecg_data, specialist_visit, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, rate, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, rhythm, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, stroke, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, est_care, on='UniqueID')\n",
    "    ecg_data = pd.merge(ecg_data, insurance, on='UniqueID')\n",
    "print(\"\\n# of Patients in  sample: \", ecg_data['UniqueID'].nunique())\n",
    "print(\"# of ECGs in sample: \", len(ecg_data))\n",
    "\n",
    "ecg_data['DeathDTS'] = pd.to_datetime(ecg_data['DeathDTS']) \n",
    "ecg_data['date'] = pd.to_datetime(ecg_data['date']) \n",
    "ecg_data['mortality_within_one_year'] =  (ecg_data['DeathDTS'] - ecg_data['date']).dt.days < 365\n",
    "\n",
    "# Remove UniqueIDs who are associated wi\n",
    "# th both positive & negative class; it's because of middle name\n",
    "uniqueids_positive_and_negative = ecg_data.groupby('UniqueID')['label'].nunique()\n",
    "repeated_unique_ids_across_class = uniqueids_positive_and_negative[uniqueids_positive_and_negative > 1].index.values\n",
    "ecg_data = ecg_data[~ecg_data['UniqueID'].isin(repeated_unique_ids_across_class)]\n",
    "\n",
    "if preprocessing_params['one_ecg_per_patient'] != 'false':\n",
    "    # Sort by ECG date (ascending=False)\n",
    "    # Select one per UniqueID\n",
    "    ecg_data = ecg_data.sort_values('ecg_date')\n",
    "    if preprocessing_params['one_ecg_per_patient'].startswith('first'):\n",
    "        ecg_data = ecg_data.groupby('UniqueID').first().reset_index()\n",
    "    else:\n",
    "        ecg_data = ecg_data.groupby('UniqueID').last().reset_index()\n",
    "print(ecg_data['label'].mean(), preprocessing_params)\n",
    "print(\"\\n# of Patients in  sample: \", ecg_data['UniqueID'].nunique())\n",
    "print(\"# of ECGs in sample: \", len(ecg_data))\n",
    "\n",
    "ecg_data['PrimLangDSC'] = ecg_data['PrimLangDSC'].map(lambda x: x if x == \"English\" else \"Non-English\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train, calibration, and study sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id_col = 'UniqueID'\n",
    "additional_feat_names = ['binary_Black or African American', 'binary_Hispanic or Latino',\n",
    "                         'binary_Declined or Unavailable','binary_Asian',  'binary_Other', \n",
    "                         'binary_Native American or Pacific Islander',\n",
    "                         'binary_Male', \n",
    "                         'PatientAge_years_01']\n",
    "# additional_feat_names = []\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 6\n",
    "if preprocessing_params['mini']:\n",
    "    train_ids = np.random.choice(train_ids, size=int(.05*len(train_ids)), replace=False)\n",
    "    \n",
    "split_dfs = []\n",
    "split_paths = []\n",
    "split_y = []\n",
    "additional_feats = []\n",
    "for i, pid_set in enumerate([train_ids, val_ids, test_ids]):\n",
    "    split_df = ecg_data[ecg_data[unique_id_col].isin(pid_set)]\n",
    "    split_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if i in [0,1] and preprocessing_params['one_ecg_per_patient'] == 'last_white':\n",
    "        split_df = split_df[split_df['PatientRaceFinal'] == 'White']\n",
    "\n",
    "    if i in [0,1] and preprocessing_params['one_ecg_per_patient'] == 'last_two_ecgs':\n",
    "        split_df = split_df[split_df['UniqueID'].isin(pids_to_keep_under_two_ecg_constraint)]\n",
    "\n",
    "    if i > 0:\n",
    "        # Make sure we're evaluating each patient with only one ECG\n",
    "        split_df = split_df.sample(frac=1, random_state=0)\n",
    "        split_df = split_df.groupby(unique_id_col).first().reset_index() # Replace with a random ECG\n",
    "\n",
    "        # \n",
    "        split_df = split_df[split_df[unique_id_col].isin(unique_ids_in_new_system)]\n",
    "    split_paths.append(list(split_df['path_to_bwr_trunc_norm_data']))\n",
    "    split_y.append(np.array(list(split_df['label'])))\n",
    "    additional_feat_values = split_df[additional_feat_names].fillna(0).values\n",
    "    split_dfs.append(split_df)\n",
    "    additional_feats.append(additional_feat_values.astype(int))\n",
    "\n",
    "train_ecg_paths, val_ecg_paths, test_ecg_paths = split_paths\n",
    "train_additional_feats, val_additional_feats, test_additional_feats = additional_feats\n",
    "train_y, val_y, test_y = split_y\n",
    "\n",
    "if len(additional_feat_names) == 0:\n",
    "    train_dataset = ECGDataset(train_ecg_paths, train_y)\n",
    "    val_dataset = ECGDataset(val_ecg_paths, val_y)\n",
    "    test_dataset = ECGDataset(test_ecg_paths, test_y)\n",
    "else:\n",
    "    train_dataset = ECGDemographicsDataset(train_ecg_paths,  train_additional_feats, train_y)\n",
    "    val_dataset = ECGDemographicsDataset(val_ecg_paths, val_additional_feats, val_y)\n",
    "    test_dataset = ECGDemographicsDataset(test_ecg_paths, test_additional_feats, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=False, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=False, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=False, shuffle=False, num_workers=num_workers)\n",
    "print(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset))\n",
    "\n",
    "expt_config={'arch': 'Net1D','additional_features': False}\n",
    "if len(additional_feat_names) > 0:\n",
    "    expt_config['additional_features'] = True\n",
    "\n",
    "train_y.mean(), val_y.mean(), test_y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data = pd.concat(split_dfs)\n",
    "\n",
    "# Redo splits, as is done in train_models.ipynb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "instype_map = {'Medicaid': 'Medicaid', 'Unknown/Missing': 'Unknown/Missing', 'Commercial': 'Commercial', 'Dual': 'Medicare', 'Medicare':'Medicare','Other': 'Other' }\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "age_thresh = 17\n",
    "val_split_df = split_dfs[1]\n",
    "test_split_df = split_dfs[2]\n",
    "train_split_df = split_dfs[0]\n",
    "\n",
    "combined_df = pd.concat([val_split_df, test_split_df], ignore_index=True)\n",
    "shuffled_df = shuffle(combined_df, random_state=42)\n",
    "\n",
    "# Split back into validation and test sets\n",
    "split_ratio = 0.625\n",
    "split_point = int(len(shuffled_df) * split_ratio)\n",
    "\n",
    "# Create new validation and test splits\n",
    "val_split_df = shuffled_df.iloc[:split_point].reset_index(drop=True)\n",
    "test_split_df = shuffled_df.iloc[split_point:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out latex variables referenced in paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output latex macros\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\nEntireSample}}{ {np.sum([len(df) for df in split_dfs])}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nPosEntireSample}}{ {np.sum([df['label'].sum() for df in split_dfs])}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nNegEntireSample}}{ {np.sum([len(df[df['label']== 0]) for df in split_dfs])}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\nTrain}}{ {len(split_dfs[0])}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nCal}}{ {len(val_split_df)}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nStudy}}{ {len(test_split_df)}}\")\n",
    "\n",
    "pos_ecgs = ecg_data[ecg_data['label'] == 1] \n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\nDiagEntireSample}}{ {ecg_data['diagnosis_in_charts'].sum()}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nDiagAmongAFPos}}{ {pos_ecgs['diagnosis_in_charts'].sum()}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nUnDiagAmongAFPos}}{ {len(pos_ecgs[pos_ecgs['diagnosis_in_charts'] == 0])}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nDiagWhite}}{ {ecg_data[ecg_data['PatientRaceFinal'] == 'White']['diagnosis_in_charts'].sum()}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nDiagBlack}}{ {ecg_data[ecg_data['PatientRaceFinal'] == 'Black or African American']['diagnosis_in_charts'].sum()}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nDiagAsian}}{ {ecg_data[ecg_data['PatientRaceFinal'] == 'Asian']['diagnosis_in_charts'].sum()}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\nDiagHisp}}{ {ecg_data[ecg_data['PatientRaceFinal'] == 'Hispanic or Latino']['diagnosis_in_charts'].sum()}}\")\n",
    "\n",
    "pos_ecg_with_PCP = pos_ecgs[pos_ecgs['PCPvisits_bin'] == 1]\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\nPosWithPCPEntireSample}}{ {len(pos_ecg_with_PCP)}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nDiagAmongPosWithPCPEntireSample}}{  {pos_ecg_with_PCP['diagnosis_in_charts'].sum()}}\")\n",
    "print(f\"\\\\newcommand{{\\\\nUnDiagAmongPosWithPCPEntireSample}}{ {len(pos_ecg_with_PCP[pos_ecg_with_PCP['diagnosis_in_charts'] == 0])}}\")\n",
    "\n",
    "\n",
    "# Compute black-specific diagnosis rates\n",
    "# Compute white-specific  diagnosis rates\n",
    "pos_ecg_with_PCP['no_diagnosis_in_chart'] = ~(pos_ecg_with_PCP['diagnosis_in_charts'].astype(bool))\n",
    "race_to_hidden_diag_rates = pos_ecg_with_PCP.groupby('PatientRaceFinal')['no_diagnosis_in_chart'].mean().reset_index()\n",
    "ins_to_hidden_diag_rates = pos_ecg_with_PCP.groupby('instype_final')['no_diagnosis_in_chart'].mean().reset_index()\n",
    "lang_to_hidden_diag_rates = pos_ecg_with_PCP.groupby('PrimLangDSC')['no_diagnosis_in_chart'].mean().reset_index()\n",
    "\n",
    "hidden_diag_rate_white = race_to_hidden_diag_rates[race_to_hidden_diag_rates['PatientRaceFinal'] == 'White']['no_diagnosis_in_chart'].iloc[0]\n",
    "hidden_diag_rate_black = race_to_hidden_diag_rates[race_to_hidden_diag_rates['PatientRaceFinal'] == 'Black or African American']['no_diagnosis_in_chart'].iloc[0]\n",
    "hidden_diag_rate_comm = ins_to_hidden_diag_rates[ins_to_hidden_diag_rates['instype_final'] == 'Commercial']['no_diagnosis_in_chart'].iloc[0]\n",
    "hidden_diag_rate_medicaid = ins_to_hidden_diag_rates[ins_to_hidden_diag_rates['instype_final'] == 'Medicaid']['no_diagnosis_in_chart'].iloc[0]\n",
    "hidden_diag_rate_eng = lang_to_hidden_diag_rates[lang_to_hidden_diag_rates['PrimLangDSC'] == 'English']['no_diagnosis_in_chart'].iloc[0]\n",
    "hidden_diag_rate_noneng = lang_to_hidden_diag_rates[lang_to_hidden_diag_rates['PrimLangDSC'] == 'Non-English']['no_diagnosis_in_chart'].iloc[0]\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\pctHiddenDiagWhitePosPCP}}{ {np.round(100*hidden_diag_rate_white, 1)}}\")\n",
    "print(f\"\\\\newcommand{{\\\\pctHiddenDiagBlackPosPCP}}{ {np.round(100*hidden_diag_rate_black, 1)}}\")\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\pctHiddenDiagCommercialPosPCP}}{ {np.round(100*hidden_diag_rate_comm, 1)}}\")\n",
    "print(f\"\\\\newcommand{{\\\\pctHiddenDiagMedicaidPosPCP}}{ {np.round(100*hidden_diag_rate_medicaid, 1)}}\")\n",
    "\n",
    "\n",
    "print(f\"\\\\newcommand{{\\\\pctHiddenDiagEngPosPCP}}{ {np.round(100*hidden_diag_rate_eng, 1)}}\")\n",
    "print(f\"\\\\newcommand{{\\\\pctHiddenDiagNongEngPosPCP}}{ {np.round(100*hidden_diag_rate_noneng, 1)}}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_to_hidden_diag_rates = pos_ecg_with_PCP.groupby('PrimLangDSC')['no_diagnosis_in_chart'].mean().reset_index()\n",
    "lang_to_hidden_diag_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Table S1, summary statistics of three samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from utils import prettify_group_name, prettify_col_name\n",
    "category_to_values = {'SexDSC': ('Male', 'Female'),\n",
    "                      'PatientRaceFinal': ('White', 'Black or African American', 'Hispanic or Latino', 'Asian'),\n",
    "                      'instype_final': ('Commercial', 'Medicare', 'Medicaid' ),\n",
    "                      'PrimLangDSC': ('English', 'Non-English')}\n",
    "\n",
    "def generate_table1(train_split_df, val_split_df, test_split_df):\n",
    "    # Define the splits\n",
    "    splits = {\n",
    "        \"Train\": train_split_df,\n",
    "        \"Calibration\": val_split_df,\n",
    "        \"Study\": test_split_df\n",
    "    }\n",
    "\n",
    "    # Initialize an empty dictionary to store rows\n",
    "    table_data = []\n",
    "\n",
    "    # Row 1: Number of Patients\n",
    "    num_patients = {name: len(df) for name, df in splits.items()}\n",
    "    table_data.append([''] + [\"# of Patients\"] + [f\"{num_patients[name]}\" for name in splits.keys()])\n",
    "\n",
    "    # Row 2: Age (Mean (SD))\n",
    "    age_summary = {\n",
    "        name: f\"{df['PatientAge_years'].mean():.1f} ({df['PatientAge_years'].std():.1f})\" for name, df in splits.items()\n",
    "    }\n",
    "    table_data.append([''] + [\"Age (Mean (SD))\"] + [age_summary[name] for name in splits.keys()])\n",
    "\n",
    "    # Rows for categorical variables\n",
    "    categorical_vars = [\"SexDSC\", \"PatientRaceFinal\", \"instype_final\", \"PrimLangDSC\"]\n",
    "    for var in categorical_vars:\n",
    "        unique_values = category_to_values[var]\n",
    "        pretty_var = prettify_group_name[var]\n",
    "        for i, value in enumerate(unique_values):\n",
    "            row = [f'{pretty_var}' if i == 0 else ''] + [ str(value)] + [\n",
    "                f\"{len(df[df[var] == value])} ({((df[var] == value).mean() * 100):.1f}%)\" for name, df in splits.items()\n",
    "            ]\n",
    "            table_data.append(row)\n",
    "    # Rows for binary variables\n",
    "    binary_vars = [\"diagnosis_in_charts\", \"label\",  \"stroke\"]\n",
    "    for i, var in enumerate(binary_vars):\n",
    "        pretty_var = prettify_col_name(var)\n",
    "        row = ['Outcome' if i == 0 else ''] + [pretty_var] + [\n",
    "            f\"{df[var].sum()} ({(df[var].mean() * 100):.1f}%)\" for name, df in splits.items()\n",
    "        ]\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Convert to a DataFrame for formatting\n",
    "    table_df = pd.DataFrame(table_data, columns=[\"\"] + [\"\"] + list(splits.keys()))\n",
    "\n",
    "    # Convert to LaTeX using tabulate\n",
    "    latex_table = tabulate(table_df, headers=\"keys\", tablefmt=\"latex\", showindex=False)\n",
    "    return latex_table, table_data\n",
    "\n",
    "# Generate the table\n",
    "latex_table, table_data = generate_table1(split_dfs[0], val_split_df, test_split_df)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Your table data as list of lists\n",
    "\n",
    "# Formatting helpers\n",
    "midrule_labels = {\"Sex\", \"Race\", \"Insurance\", \"Primary Language\", \"Outcome\"}\n",
    "indented_labels = {\"Male\", \"Female\", \"White\", \"Black or African American\", \"Hispanic or Latino\", \"Asian\",\n",
    "                   \"Commercial\", \"Medicare\", \"Medicaid\", \"English\", \"Non-English\"}\n",
    "bold_labels = midrule_labels\n",
    "\n",
    "def format_number(s):\n",
    "    return re.sub(r'\\d{1,3}(?=(\\d{3})+(?!\\d))', r'\\g<0>,', s)\n",
    "\n",
    "def escape_latex(s):\n",
    "    s = s.replace('%', r'\\%')\n",
    "    s = re.sub(r'<', r'\\\\ensuremath{<}', s)\n",
    "    s = re.sub(r'>', r'\\\\ensuremath{>}', s)\n",
    "    return s\n",
    "\n",
    "def format_cell(s):\n",
    "    s = format_number(s)\n",
    "    s = escape_latex(s)\n",
    "    return s\n",
    "\n",
    "# Build LaTeX table\n",
    "latex_lines = []\n",
    "latex_lines.append(r\"\\begin{tabular}{lrrrr}\")\n",
    "latex_lines.append(r\"\\toprule\")\n",
    "latex_lines.append(r\"& & \\textbf{Train} & \\textbf{Calibration} & \\textbf{Study}\")\n",
    "latex_lines.append(r\"\\midrule\")\n",
    "for row in table_data:\n",
    "    label = row[0]\n",
    "    \n",
    "    # Add \\midrule before new sections\n",
    "    if label in midrule_labels:\n",
    "        latex_lines.append(r\"\\midrule\")\n",
    "\n",
    "    # Indent certain labels\n",
    "    if label in indented_labels:\n",
    "        label = r\"\\quad \" + label\n",
    "\n",
    "    # Bold section headers\n",
    "    if label.strip() in bold_labels:\n",
    "        label = r\"\\textbf{\" + label.strip() + r\"}\"\n",
    "\n",
    "    # Format row\n",
    "    formatted_row = \" & \".join([label] + [format_cell(cell) for cell in row[1:]]) + r\" \\\\\"\n",
    "    latex_lines.append(formatted_row)\n",
    "\n",
    "latex_lines.append(r\"\\bottomrule\")\n",
    "latex_lines.append(r\"\\end{tabular}\")\n",
    "\n",
    "# Output LaTeX\n",
    "latex_output = \"\\n\".join(latex_lines)\n",
    "print(latex_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "def format_percent_with_ci(positives, total, alpha=0.05, method='wilson'):\n",
    "    if total == 0:\n",
    "        return \"N/A\"\n",
    "\n",
    "    point_estimate = positives / total\n",
    "    ci_low, ci_high = proportion_confint(count=positives, nobs=total, alpha=alpha, method=method)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    pe_pct = round(point_estimate * 100, 1)\n",
    "    ci_low_pct = round(ci_low * 100, 1)\n",
    "    ci_high_pct = round(ci_high * 100, 1)\n",
    "\n",
    "    return f\"{pe_pct} ({ci_low_pct}, {ci_high_pct})\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "n_pos_ecgs = len(pos_ecgs)\n",
    "n_pos_ecgs_pcp = len(pos_ecg_with_PCP)\n",
    "hidden_diag_rate_among_pos = 100 - np.round(pos_ecgs['diagnosis_in_charts'].mean()*100, 2)\n",
    "hidden_diag_rate_among_pos_with_pcp = 100 - np.round(pos_ecg_with_PCP['diagnosis_in_charts'].mean()*100, 2)\n",
    "\n",
    "hidden_diag_n_among_pos = pos_ecgs['diagnosis_in_charts'].value_counts()[False]\n",
    "hidden_diag_n_among_pos_with_pcp = pos_ecg_with_PCP['diagnosis_in_charts'].value_counts()[False]\n",
    "\n",
    "pos_ecg_ci = format_percent_with_ci(hidden_diag_n_among_pos, n_pos_ecgs)\n",
    "pos_ecg_with_pcp_ci = format_percent_with_ci(hidden_diag_n_among_pos_with_pcp, n_pos_ecgs_pcp)\n",
    "\n",
    "rows.append({'Group': f\"Overall = --\", #'% EHR Diagnosis': f'{diag_rate:.1f}' , \n",
    "                    '% Hidden Diagnosis (AF+)': pos_ecg_ci, \n",
    "                    '% Hidden Diagnosis (AF+, PCP)': pos_ecg_with_pcp_ci})\n",
    "\n",
    "# rows.append({'Group':f\"Overall = --\", #'% EHR Diagnosis': f'{diag_rate:.1f}' , \n",
    "#                     '% EHR Diagnosis (AF+)': f'{hidden_diag_rate_among_pos:.1f} ({hidden_diag_n_among_pos})', \n",
    "#                     '% EHR Diagnosis (AF+, PCP)': f'{hidden_diag_rate_among_pos_with_pcp:.1f} ({hidden_diag_n_among_pos_with_pcp})'})\n",
    "          \n",
    "# Iterate over groups and values\n",
    "for group, values in category_to_values.items():\n",
    "    if prettify_group_name[group] == 'Sex':\n",
    "        continue \n",
    "    for val in values:\n",
    "        # diag_rate = np.round(ecg_data[ecg_data[group] == val]['diagnosis_in_charts'].mean()*100,2)\n",
    "        hidden_diag_rate_among_pos = 100 - np.round(pos_ecgs[pos_ecgs[group] == val]['diagnosis_in_charts'].mean()*100, 2)\n",
    "        hidden_diag_rate_among_pos_with_pcp = 100 - np.round(pos_ecg_with_PCP[pos_ecg_with_PCP[group] == val]['diagnosis_in_charts'].mean()*100, 2)\n",
    "\n",
    "        hidden_diag_n_among_pos = pos_ecgs[pos_ecgs[group] == val]['diagnosis_in_charts'].value_counts()[False]\n",
    "        hidden_diag_n_among_pos_with_pcp = pos_ecg_with_PCP[pos_ecg_with_PCP[group] == val]['diagnosis_in_charts'].value_counts()[False]\n",
    "\n",
    "        pos_ecg_ci = format_percent_with_ci(hidden_diag_n_among_pos, len(pos_ecgs[pos_ecgs[group] == val]))\n",
    "        pos_ecg_with_pcp_ci = format_percent_with_ci(hidden_diag_n_among_pos_with_pcp, len(pos_ecg_with_PCP[pos_ecg_with_PCP[group] == val]))\n",
    "\n",
    "\n",
    "        rows.append({'Group': f\"{prettify_group_name[group]} = {val}\", #'% EHR Diagnosis': f'{diag_rate:.1f}' , \n",
    "                    '% Hidden Diagnosis (AF+)': pos_ecg_ci + ' ' + str(hidden_diag_rate_among_pos), \n",
    "                    '% Hidden Diagnosis (AF+, PCP)': pos_ecg_with_pcp_ci + ' ' + str(hidden_diag_rate_among_pos_with_pcp)})\n",
    "        \n",
    "        # rows.append({'Group': f\"{prettify_group_name[group]} = {val}\", #'% EHR Diagnosis': f'{diag_rate:.1f}' , \n",
    "        #             '% EHR Diagnosis (AF+)': f'{hidden_diag_rate_among_pos:.1f} ({hidden_diag_n_among_pos})', \n",
    "        #             '% EHR Diagnosis (AF+, PCP)': f'{hidden_diag_rate_among_pos_with_pcp:.1f} ({hidden_diag_n_among_pos_with_pcp})'})\n",
    "        \n",
    "diagnosis_rates = pd.DataFrame(rows)\n",
    "diagnosis_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for display name cleanup\n",
    "value_rename_map = {\n",
    "    'BlackorAfricanAmerican': 'Black/African American',\n",
    "    'HispanicorLatino': 'Hispanic/Latino'\n",
    "}\n",
    "\n",
    "# Start LaTeX table (requires \\usepackage{makecell} in LaTeX preamble)\n",
    "latex_rows = []\n",
    "latex_rows.append(r\"\\begin{tabular}{llccc}\")\n",
    "latex_rows.append(r\"\\toprule\")\n",
    "latex_rows.append(r\" & & \\makecell{\\textbf{Patients with}\\\\\\textbf{AF ECG}} & \\makecell{\\textbf{Patients with}\\\\\\textbf{AF ECG, PCP Visit}} \\\\\")\n",
    "latex_rows.append(r\" & & \\% Hidden Diagnosis (CI) & \\% Hidden DIagnosis (CI) \\\\\")\n",
    "latex_rows.append(r\"\\hline\")\n",
    "\n",
    "prev_category = None\n",
    "for _, row in diagnosis_rates.iterrows():\n",
    "    group_info = row['Group']\n",
    "    category, val = group_info.split('=')\n",
    "    category = category.strip()\n",
    "    val = val.strip().replace(' ', '')\n",
    "\n",
    "    # Clean display values\n",
    "    val = value_rename_map.get(val, val)\n",
    "\n",
    "    # Only show category label the first time it appears\n",
    "    group_col = f\"\\\\textbf{{{category}}}\" if category != prev_category else \"\"\n",
    "    if category != prev_category:\n",
    "        latex_rows.append(\"\\midrule\")\n",
    "    prev_category = category\n",
    "\n",
    "    # Build LaTeX row\n",
    "    latex_row = f\"{group_col} & {val}  & {row['% Hidden Diagnosis (AF+)']} & {row['% Hidden Diagnosis (AF+, PCP)']} \\\\\\\\\"\n",
    "    latex_rows.append(latex_row)\n",
    "latex_rows.append(r\"\\bottomrule\")\n",
    "latex_rows.append(r\"\\end{tabular}\")\n",
    "\n",
    "# Combine and print\n",
    "latex_code = \"\\n\".join(latex_rows)\n",
    "print(latex_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
